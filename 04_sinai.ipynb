{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluation\n",
    "Perform two evaluations:\n",
    "1. Strict morpheme evaluation\n",
    "1. Token evaluation (morpheme labels are extended to the token level heuristically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:20:24.034028Z",
     "start_time": "2019-03-13T07:20:22.687626Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:20:24.382406Z",
     "start_time": "2019-03-13T07:20:24.037019Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:20:27.996727Z",
     "start_time": "2019-03-13T07:20:24.385088Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/nlp/danb/NER')\n",
    "\n",
    "import bclm\n",
    "import ne_evaluate_mentions as nem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../wiki_dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "clean_nikud=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hebtokenizer import tokenize\n",
    "from hebtokenizer import alternative_scanner\n",
    "alt_scan = True\n",
    "clean_junk = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hebchars(text):\n",
    "    norm = unicodedata.normalize('NFKD', text)\n",
    "    text =''.join([c for c in norm if not unicodedata.combining(c)]) \n",
    "    #line = line.replace('־', '-')\n",
    "    # maqaf\n",
    "    text = text.replace(u'\\u05be', '-')\n",
    "\n",
    "    #line = line.replace('׳', '\\'')\n",
    "    #geresh\n",
    "    text = text.replace(u'\\u05f3', '\\'')\n",
    "\n",
    "    #line = line.replace('״', '\"')\n",
    "    #gershayim\n",
    "    text = text.replace(u'\\u05f4', '\"')\n",
    "    #line = line.replace('”', '\"')\n",
    "    #line = line.replace('„', '\"')\n",
    "    #en dash\n",
    "    text = text.replace(u'\\u2013', '-')\n",
    "    #em dash\n",
    "    text = text.replace(u'\\u2014', '-')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sents):\n",
    "    tok_sents = []\n",
    "    for sent in sents:\n",
    "        if alt_scan:\n",
    "            tok = tokenize(sent, alternative_scanner)\n",
    "        else:\n",
    "            tok = tokenize(sent)\n",
    "\n",
    "        last_type, last_form = tok[-1]\n",
    "        if len(last_form)>1 and last_type!='PUNCT' and last_form[-1] in ('?', '!', '.'):\n",
    "            tok[-1] = (last_type, last_form[:-1])\n",
    "            tok.append(('PUNCT', last_form[-1]))\n",
    "\n",
    "        final = []\n",
    "        for c, t in tok: \n",
    "            if clean_junk and c=='JUNK':\n",
    "                continue\n",
    "            final.append(t)\n",
    "        tok_sents.append(final)\n",
    "    return tok_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['התתר אשר היה במעקקא והשתטח על קבר היותו במעקקא יעטוף על מצנפתו חגור לבן או אדום  וישאהו כל הימים',\n",
       " 'כל איש אשר התבונן בעין בחונה אל הנעשה, והנשמע במשך השנים האחרונות  בארצות הדאנוי, ראה כי דעת הקהל התהפכה שם  כחומר חותם ורוח אחרת היתה אתה',\n",
       " 'גם זה היה לטובה להאורחה ההיא, כי נספחו אליה שני שעריפים (צאצאי מחמד) מעיר אל-שרק הנמצאה בין החוף והמדבר, אשר שבו מדרך הקודש, מערי מעקקא ומודינה, וילכו לשוב אל עירם  ומולדתם',\n",
       " 'לא טוב גורל האובדים והנדחים בפאריז, ולא טובה ממנו גם מנת חלק רוב הפליטים המתגוררים בארץ הצבי',\n",
       " 'לפי הודעת מכ\"ע האנגלים כבר מצאה הממשלה מספר אניות הדרושות אל החפץ, ובעת הזאת מן גדות הטהעמזע ומן גדות הגאנגעס יעופו צבאות אנגליא המעטים וטובים כעל כנפי נשרים לעשות נפלאות בארץ חם,  בפרט ע\"ד צבאות הודו לא ייעפו ולא ייגעו מכה\"ע האנגלים  לתנות תקף מעשיהם וגבורתם הנפלאה',\n",
       " \"גם ביחוס פלגות הארץ לפי מזגה יחלקה ה' בורטאן לשני חלקים: החלק הצפוני עם החוף מאקווא עוד לא נחקר ולא נדרש כל צרכו; ובחלק הדרומי אשד לו החוף וועדי, תתנכרנה עקבות עמי קדם אשר היו שם\",\n",
       " 'בראשית מסענו לא פגשנו איש, שקט ודומיה שררו בכל הככר, אך מדי הוספנו ללכת, ראינו אותות חיים, וכל חדרך המה מהמון אנשים ונשים וטף, מרביתם ערביים משבט הפעללאכים מכוסים בבלויי סחבות, ונושאים על שכמם סלים מלאים עפר וחול דק, להביא אל הכפר גיצע',\n",
       " 'הוא עבר את גבול עטרעק, את המדבר הגדול והנורא מעון שודדי הטורקמאנען עד היווא; משם  אחז את דרכו בוכאראה, זאמארקאנד וקארשי, ויפתח את שערי הארצות האל לפני חכמי ידיעות העתים (עטהנאגראפהיע) ואך להחכם הזה אשר שם נפשו בכפו וישחק לכל עמל ותלאה אשר  אשר מצאוהו על דרכו זה, עלינו להודות על כל אשר נדע מהארצות האל',\n",
       " 'אולם משה השכיל למצוא תחבולה ויצו לאנשי צבאותיו לגדל תרבות עוף החסידה, ואחרי כן נטשו את החסידות על פני בארות החמר ותנקרנה את הפתנים ותבערנה אחריהם עד תומם, וגם הצליח חפצו בידו, ויפרוץ משה עם צבאותיו על מחנה הכושים ויכם ויכתם ואת שרידי חרבו רדף עד העיר סאבא',\n",
       " 'כל מכה\"ע היוצאים לאור באייראפא המערבית, ובפרט מ\"ע הצרפתים והאנגלים, עסוקים וטרודים הם ביחוד בדבר המלחמה בכינא: זה כשבועים אין אף גליון אחד אשר לא נמצא בו מאמרים  ,ראשים הכוללים הגיוני מכתב העתי, או מכתבים ממערכות המלחמה או מאמרי ויכוח נגד שאר מכ\"ע']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = []\n",
    "for line in open('../NER/data/sinai/HZF-sentences.tsv'):\n",
    "    sent = clean_hebchars(line.strip())\n",
    "    if sent=='text':\n",
    "        continue\n",
    "    sents.append(sent)\n",
    "\n",
    "sents[:10]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['התתר',\n",
       "  'אשר',\n",
       "  'היה',\n",
       "  'במעקקא',\n",
       "  'והשתטח',\n",
       "  'על',\n",
       "  'קבר',\n",
       "  'היותו',\n",
       "  'במעקקא',\n",
       "  'יעטוף',\n",
       "  'על',\n",
       "  'מצנפתו',\n",
       "  'חגור',\n",
       "  'לבן',\n",
       "  'או',\n",
       "  'אדום',\n",
       "  'וישאהו',\n",
       "  'כל',\n",
       "  'הימים'],\n",
       " ['כל',\n",
       "  'איש',\n",
       "  'אשר',\n",
       "  'התבונן',\n",
       "  'בעין',\n",
       "  'בחונה',\n",
       "  'אל',\n",
       "  'הנעשה',\n",
       "  ',',\n",
       "  'והנשמע',\n",
       "  'במשך',\n",
       "  'השנים',\n",
       "  'האחרונות',\n",
       "  'בארצות',\n",
       "  'הדאנוי',\n",
       "  ',',\n",
       "  'ראה',\n",
       "  'כי',\n",
       "  'דעת',\n",
       "  'הקהל',\n",
       "  'התהפכה',\n",
       "  'שם',\n",
       "  'כחומר',\n",
       "  'חותם',\n",
       "  'ורוח',\n",
       "  'אחרת',\n",
       "  'היתה',\n",
       "  'אתה'],\n",
       " ['גם',\n",
       "  'זה',\n",
       "  'היה',\n",
       "  'לטובה',\n",
       "  'להאורחה',\n",
       "  'ההיא',\n",
       "  ',',\n",
       "  'כי',\n",
       "  'נספחו',\n",
       "  'אליה',\n",
       "  'שני',\n",
       "  'שעריפים',\n",
       "  '(',\n",
       "  'צאצאי',\n",
       "  'מחמד',\n",
       "  ')',\n",
       "  'מעיר',\n",
       "  'אל',\n",
       "  '-',\n",
       "  'שרק',\n",
       "  'הנמצאה',\n",
       "  'בין',\n",
       "  'החוף',\n",
       "  'והמדבר',\n",
       "  ',',\n",
       "  'אשר',\n",
       "  'שבו',\n",
       "  'מדרך',\n",
       "  'הקודש',\n",
       "  ',',\n",
       "  'מערי',\n",
       "  'מעקקא',\n",
       "  'ומודינה',\n",
       "  ',',\n",
       "  'וילכו',\n",
       "  'לשוב',\n",
       "  'אל',\n",
       "  'עירם',\n",
       "  'ומולדתם'],\n",
       " ['לא',\n",
       "  'טוב',\n",
       "  'גורל',\n",
       "  'האובדים',\n",
       "  'והנדחים',\n",
       "  'בפאריז',\n",
       "  ',',\n",
       "  'ולא',\n",
       "  'טובה',\n",
       "  'ממנו',\n",
       "  'גם',\n",
       "  'מנת',\n",
       "  'חלק',\n",
       "  'רוב',\n",
       "  'הפליטים',\n",
       "  'המתגוררים',\n",
       "  'בארץ',\n",
       "  'הצבי'],\n",
       " ['לפי',\n",
       "  'הודעת',\n",
       "  'מכ\"ע',\n",
       "  'האנגלים',\n",
       "  'כבר',\n",
       "  'מצאה',\n",
       "  'הממשלה',\n",
       "  'מספר',\n",
       "  'אניות',\n",
       "  'הדרושות',\n",
       "  'אל',\n",
       "  'החפץ',\n",
       "  ',',\n",
       "  'ובעת',\n",
       "  'הזאת',\n",
       "  'מן',\n",
       "  'גדות',\n",
       "  'הטהעמזע',\n",
       "  'ומן',\n",
       "  'גדות',\n",
       "  'הגאנגעס',\n",
       "  'יעופו',\n",
       "  'צבאות',\n",
       "  'אנגליא',\n",
       "  'המעטים',\n",
       "  'וטובים',\n",
       "  'כעל',\n",
       "  'כנפי',\n",
       "  'נשרים',\n",
       "  'לעשות',\n",
       "  'נפלאות',\n",
       "  'בארץ',\n",
       "  'חם',\n",
       "  ',',\n",
       "  'בפרט',\n",
       "  'ע\"ד',\n",
       "  'צבאות',\n",
       "  'הודו',\n",
       "  'לא',\n",
       "  'ייעפו',\n",
       "  'ולא',\n",
       "  'ייגעו',\n",
       "  'מכה\"ע',\n",
       "  'האנגלים',\n",
       "  'לתנות',\n",
       "  'תקף',\n",
       "  'מעשיהם',\n",
       "  'וגבורתם',\n",
       "  'הנפלאה'],\n",
       " ['גם',\n",
       "  'ביחוס',\n",
       "  'פלגות',\n",
       "  'הארץ',\n",
       "  'לפי',\n",
       "  'מזגה',\n",
       "  'יחלקה',\n",
       "  \"ה'\",\n",
       "  'בורטאן',\n",
       "  'לשני',\n",
       "  'חלקים',\n",
       "  ':',\n",
       "  'החלק',\n",
       "  'הצפוני',\n",
       "  'עם',\n",
       "  'החוף',\n",
       "  'מאקווא',\n",
       "  'עוד',\n",
       "  'לא',\n",
       "  'נחקר',\n",
       "  'ולא',\n",
       "  'נדרש',\n",
       "  'כל',\n",
       "  'צרכו',\n",
       "  ';',\n",
       "  'ובחלק',\n",
       "  'הדרומי',\n",
       "  'אשד',\n",
       "  'לו',\n",
       "  'החוף',\n",
       "  'וועדי',\n",
       "  ',',\n",
       "  'תתנכרנה',\n",
       "  'עקבות',\n",
       "  'עמי',\n",
       "  'קדם',\n",
       "  'אשר',\n",
       "  'היו',\n",
       "  'שם'],\n",
       " ['בראשית',\n",
       "  'מסענו',\n",
       "  'לא',\n",
       "  'פגשנו',\n",
       "  'איש',\n",
       "  ',',\n",
       "  'שקט',\n",
       "  'ודומיה',\n",
       "  'שררו',\n",
       "  'בכל',\n",
       "  'הככר',\n",
       "  ',',\n",
       "  'אך',\n",
       "  'מדי',\n",
       "  'הוספנו',\n",
       "  'ללכת',\n",
       "  ',',\n",
       "  'ראינו',\n",
       "  'אותות',\n",
       "  'חיים',\n",
       "  ',',\n",
       "  'וכל',\n",
       "  'חדרך',\n",
       "  'המה',\n",
       "  'מהמון',\n",
       "  'אנשים',\n",
       "  'ונשים',\n",
       "  'וטף',\n",
       "  ',',\n",
       "  'מרביתם',\n",
       "  'ערביים',\n",
       "  'משבט',\n",
       "  'הפעללאכים',\n",
       "  'מכוסים',\n",
       "  'בבלויי',\n",
       "  'סחבות',\n",
       "  ',',\n",
       "  'ונושאים',\n",
       "  'על',\n",
       "  'שכמם',\n",
       "  'סלים',\n",
       "  'מלאים',\n",
       "  'עפר',\n",
       "  'וחול',\n",
       "  'דק',\n",
       "  ',',\n",
       "  'להביא',\n",
       "  'אל',\n",
       "  'הכפר',\n",
       "  'גיצע'],\n",
       " ['הוא',\n",
       "  'עבר',\n",
       "  'את',\n",
       "  'גבול',\n",
       "  'עטרעק',\n",
       "  ',',\n",
       "  'את',\n",
       "  'המדבר',\n",
       "  'הגדול',\n",
       "  'והנורא',\n",
       "  'מעון',\n",
       "  'שודדי',\n",
       "  'הטורקמאנען',\n",
       "  'עד',\n",
       "  'היווא',\n",
       "  ';',\n",
       "  'משם',\n",
       "  'אחז',\n",
       "  'את',\n",
       "  'דרכו',\n",
       "  'בוכאראה',\n",
       "  ',',\n",
       "  'זאמארקאנד',\n",
       "  'וקארשי',\n",
       "  ',',\n",
       "  'ויפתח',\n",
       "  'את',\n",
       "  'שערי',\n",
       "  'הארצות',\n",
       "  'האל',\n",
       "  'לפני',\n",
       "  'חכמי',\n",
       "  'ידיעות',\n",
       "  'העתים',\n",
       "  '(',\n",
       "  'עטהנאגראפהיע',\n",
       "  ')',\n",
       "  'ואך',\n",
       "  'להחכם',\n",
       "  'הזה',\n",
       "  'אשר',\n",
       "  'שם',\n",
       "  'נפשו',\n",
       "  'בכפו',\n",
       "  'וישחק',\n",
       "  'לכל',\n",
       "  'עמל',\n",
       "  'ותלאה',\n",
       "  'אשר',\n",
       "  'אשר',\n",
       "  'מצאוהו',\n",
       "  'על',\n",
       "  'דרכו',\n",
       "  'זה',\n",
       "  ',',\n",
       "  'עלינו',\n",
       "  'להודות',\n",
       "  'על',\n",
       "  'כל',\n",
       "  'אשר',\n",
       "  'נדע',\n",
       "  'מהארצות',\n",
       "  'האל'],\n",
       " ['אולם',\n",
       "  'משה',\n",
       "  'השכיל',\n",
       "  'למצוא',\n",
       "  'תחבולה',\n",
       "  'ויצו',\n",
       "  'לאנשי',\n",
       "  'צבאותיו',\n",
       "  'לגדל',\n",
       "  'תרבות',\n",
       "  'עוף',\n",
       "  'החסידה',\n",
       "  ',',\n",
       "  'ואחרי',\n",
       "  'כן',\n",
       "  'נטשו',\n",
       "  'את',\n",
       "  'החסידות',\n",
       "  'על',\n",
       "  'פני',\n",
       "  'בארות',\n",
       "  'החמר',\n",
       "  'ותנקרנה',\n",
       "  'את',\n",
       "  'הפתנים',\n",
       "  'ותבערנה',\n",
       "  'אחריהם',\n",
       "  'עד',\n",
       "  'תומם',\n",
       "  ',',\n",
       "  'וגם',\n",
       "  'הצליח',\n",
       "  'חפצו',\n",
       "  'בידו',\n",
       "  ',',\n",
       "  'ויפרוץ',\n",
       "  'משה',\n",
       "  'עם',\n",
       "  'צבאותיו',\n",
       "  'על',\n",
       "  'מחנה',\n",
       "  'הכושים',\n",
       "  'ויכם',\n",
       "  'ויכתם',\n",
       "  'ואת',\n",
       "  'שרידי',\n",
       "  'חרבו',\n",
       "  'רדף',\n",
       "  'עד',\n",
       "  'העיר',\n",
       "  'סאבא'],\n",
       " ['כל',\n",
       "  'מכה\"ע',\n",
       "  'היוצאים',\n",
       "  'לאור',\n",
       "  'באייראפא',\n",
       "  'המערבית',\n",
       "  ',',\n",
       "  'ובפרט',\n",
       "  'מ\"ע',\n",
       "  'הצרפתים',\n",
       "  'והאנגלים',\n",
       "  ',',\n",
       "  'עסוקים',\n",
       "  'וטרודים',\n",
       "  'הם',\n",
       "  'ביחוד',\n",
       "  'בדבר',\n",
       "  'המלחמה',\n",
       "  'בכינא',\n",
       "  ':',\n",
       "  'זה',\n",
       "  'כשבועים',\n",
       "  'אין',\n",
       "  'אף',\n",
       "  'גליון',\n",
       "  'אחד',\n",
       "  'אשר',\n",
       "  'לא',\n",
       "  'נמצא',\n",
       "  'בו',\n",
       "  'מאמרים',\n",
       "  ',',\n",
       "  'ראשים',\n",
       "  'הכוללים',\n",
       "  'הגיוני',\n",
       "  'מכתב',\n",
       "  'העתי',\n",
       "  ',',\n",
       "  'או',\n",
       "  'מכתבים',\n",
       "  'ממערכות',\n",
       "  'המלחמה',\n",
       "  'או',\n",
       "  'מאמרי',\n",
       "  'ויכוח',\n",
       "  'נגד',\n",
       "  'שאר',\n",
       "  'מכ\"ע']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = tokenize_sentences(sents)\n",
    "sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../NER/data/sinai/HZF-sentences_tokenized.txt', 'w', encoding='utf8') as of:\n",
    "    for sent in sents:\n",
    "        for tok in sent:\n",
    "            of.write(tok+ '\\n')\n",
    "        of.write('\\n')\n",
    "        \n",
    "with open('../NER/data/sinai/HZF-sentences_tokenized_dummy_o.bmes', 'w', encoding='utf8') as of:\n",
    "    for sent in sents:\n",
    "        for tok in sent:\n",
    "            of.write(tok+ ' O\\n')\n",
    "        of.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BIOSE files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def jsonl_to_biose(in_path, out_path, bioul_to_biose=True):\n",
    "    sents = 0\n",
    "    with open(out_path, 'w', encoding='utf8') as of:\n",
    "        for line in open(in_path, 'r'):\n",
    "            sent = json.loads(line)\n",
    "            for word, tag in zip(sent['words'], sent['tags']):\n",
    "                if bioul_to_biose:\n",
    "                    tag = tag.replace('L-', 'E-').replace('U-', 'S-')\n",
    "                of.write(word+' '+tag+'\\n')\n",
    "            of.write('\\n')\n",
    "            sents+=1\n",
    "    print (sents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "json_path = 'output/predict_sinai/multi_heBERT_53460.json'\n",
    "output_path = json_path.replace('.json', '.bmes')\n",
    "if not os.path.exists(output_path):\n",
    "    jsonl_to_biose(json_path, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_multi_path = output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run YAP MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "yap_path = '/home/nlp/danb/yapproj/src/yap/yap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GOPATH=/home/nlp/danb/yapproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nlp/danb/yapproj/src/yap/yap - invoke yap as a standalone app or as an api server\n",
      "\n",
      "Commands:\n",
      "\n",
      "    api         start api server\n",
      "    dep         runs dependency training/parsing\n",
      "    hebma       run lexicon-based morphological analyzer on raw input\n",
      "    joint       runs joint morpho-syntactic training and parsing\n",
      "    ma          run data-driven morphological analyzer on raw input\n",
      "    md          runs standalone morphological disambiguation training and parsing\n",
      "\n",
      "Use \"/home/nlp/danb/yapproj/src/yap/yap help <command>\" for more information about a command.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{yap_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_path = '../NER/data/sinai/HZF-sentences_tokenized.txt'\n",
    "lattices_path = '../NER/data/sinai/HZF-sentences_tokenized.lattices'\n",
    "!{yap_path} hebma -raw {tokens_path} -out {lattices_path} > /dev/null 2>&1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biose_count(path, sent_id_shift=1):\n",
    "    sents = nem.read_file_sents(path, fix_multi_tag=False, sent_id_shift=sent_id_shift)\n",
    "    bc = []\n",
    "    for i, sent in sents.iteritems():\n",
    "        for j, (tok, bio) in enumerate(sent):\n",
    "            bc.append([i, j+1, tok, bio, len(bio.split('^'))])\n",
    "\n",
    "    bc = pd.DataFrame(bc, columns=['sent_id', 'token_id', 'token_str', \n",
    "                                   'biose', 'biose_count'])\n",
    "    return bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_edges(lattices, bc,\n",
    "                    non_o_only=True, keep_all_if_no_valid=True):\n",
    "    valid_edges = []\n",
    "    for (i, df), (_, biose, biose_count) in zip(lattices.groupby(['sent_id', 'token_id']), \n",
    "                                                bc[['biose', 'biose_count']].itertuples()):\n",
    "        el = df[['ID1', 'ID2']].rename(columns={'ID1': 'source', 'ID2': 'target'})\n",
    "        #min_node = [n for n,v in G.nodes(data=True) if v['since'] == 'December 2008'][0]\n",
    "\n",
    "        g = nx.from_pandas_edgelist(el, create_using=nx.DiGraph)\n",
    "        min_node = el.source.min()\n",
    "        max_node = el.target.max()\n",
    "        #print(min_node,max_node)\n",
    "        #print(biose_count)\n",
    "        if non_o_only and not '-' in biose:\n",
    "            vp = list(nx.all_simple_paths(g, min_node, max_node))\n",
    "        else:\n",
    "            vp = [path for path in nx.all_simple_paths(g, min_node, max_node, cutoff=biose_count+1) if len(path)==biose_count+1]\n",
    "        if keep_all_if_no_valid and len(vp)==0:\n",
    "             vp = nx.all_simple_paths(g, min_node, max_node)\n",
    "        for path in vp:\n",
    "            for source, target in zip(path[:-1], path[1:]):\n",
    "                valid_edges.append((i[0], i[1], source, target))\n",
    "                \n",
    "    return valid_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lattices(df, path, cols = ['ID1', 'ID2', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'token_id']):\n",
    "    with open(path, 'w', encoding='utf8') as of:\n",
    "        for _, sent in df.groupby('sent_id'):\n",
    "            for _, row in sent[cols].iterrows():\n",
    "                of.write('\\t'.join(row.astype(str).tolist())+'\\n')\n",
    "            of.write('\\n')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_lattices(lattices_path, ner_pred_path, output_path, keep_all_if_no_valid=True):\n",
    "    lat = bclm.read_lattices(lattices_path)\n",
    "    bc = get_biose_count(ner_pred_path, sent_id_shift=1)\n",
    "    valid_edges = get_valid_edges(lat, bc, non_o_only=False, keep_all_if_no_valid=keep_all_if_no_valid)\n",
    "    cols = ['sent_id', 'token_id', 'ID1', 'ID2']\n",
    "    pruned_lat = lat[lat[cols].apply(lambda x: tuple(x), axis=1).isin(valid_edges)]\n",
    "    to_lattices(pruned_lat, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_lat_path = lattices_path.replace('.lattices', '_pruned.lattices')\n",
    "prune_lattices(lattices_path, ner_multi_path, pruned_lat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_lat_path_ft = lattices_path.replace('.lattices', '_pruned_ft.lattices')\n",
    "ner_multi_path_ft = '../NCRFpp/data/sinai/decode_output/HZF_sentences.multitok.char_cnn.ft_tok.51_seed.bmes'\n",
    "prune_lattices(lattices_path, ner_multi_path_ft, pruned_lat_path_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run YAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_out, map_out, conll_out = [pruned_lat_path.replace('.lattices', suf)\n",
    "                               for suf in ['.seg', '.map', '.conll']]\n",
    "if True:#not os.path.exists(map_out):\n",
    "    !{yap_path} joint -in {pruned_lat_path} -os {seg_out} -om {map_out} -oc {conll_out} > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_out_ft, map_out_ft, conll_out_ft = [pruned_lat_path_ft.replace('.lattices', suf)\n",
    "                               for suf in ['.seg', '.map', '.conll']]\n",
    "if True:#not os.path.exists(map_out):\n",
    "    !{yap_path} joint -in {pruned_lat_path_ft} -os {seg_out_ft} -om {map_out_ft} -oc {conll_out_ft} > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>form</th>\n",
       "      <th>lemma</th>\n",
       "      <th>upostag</th>\n",
       "      <th>xpostag</th>\n",
       "      <th>feats</th>\n",
       "      <th>head</th>\n",
       "      <th>deprel</th>\n",
       "      <th>deps</th>\n",
       "      <th>misc</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ה</td>\n",
       "      <td>ה</td>\n",
       "      <td>DEF</td>\n",
       "      <td>DEF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>def</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>התתר</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>תתר</td>\n",
       "      <td>תתר</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>gen=M|num=S</td>\n",
       "      <td>14</td>\n",
       "      <td>subj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>התתר</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>אשר</td>\n",
       "      <td>אשר</td>\n",
       "      <td>CC</td>\n",
       "      <td>CC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>rcmod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>אשר</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>היה</td>\n",
       "      <td>היה</td>\n",
       "      <td>COP</td>\n",
       "      <td>COP</td>\n",
       "      <td>gen=M|num=S|per=3</td>\n",
       "      <td>7</td>\n",
       "      <td>conj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>היה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ב</td>\n",
       "      <td>ב</td>\n",
       "      <td>PREPOSITION</td>\n",
       "      <td>PREPOSITION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>prepmod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>במעקקא</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id form lemma      upostag      xpostag              feats  head   deprel  \\\n",
       "0   1    ה     ה          DEF          DEF                NaN     2      def   \n",
       "1   2  תתר   תתר           NN           NN        gen=M|num=S    14     subj   \n",
       "2   3  אשר   אשר           CC           CC                NaN     2    rcmod   \n",
       "3   4  היה   היה          COP          COP  gen=M|num=S|per=3     7     conj   \n",
       "4   5    ב     ב  PREPOSITION  PREPOSITION                NaN     4  prepmod   \n",
       "\n",
       "  deps misc  sent_id token_id token_str  \n",
       "0    _    _        1        1      התתר  \n",
       "1    _    _        1        1      התתר  \n",
       "2    _    _        1        2       אשר  \n",
       "3    _    _        1        3       היה  \n",
       "4    _    _        1        4    במעקקא  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yap_out = bclm.read_yap_output(treebank_set=None, tokens_path=tokens_path, \n",
    "                                     dep_path=conll_out,\n",
    "                                     map_path=map_out)\n",
    "yap_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>form</th>\n",
       "      <th>lemma</th>\n",
       "      <th>upostag</th>\n",
       "      <th>xpostag</th>\n",
       "      <th>feats</th>\n",
       "      <th>head</th>\n",
       "      <th>deprel</th>\n",
       "      <th>deps</th>\n",
       "      <th>misc</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>התתר</td>\n",
       "      <td>התתר</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>gen=M|num=S</td>\n",
       "      <td>12</td>\n",
       "      <td>subj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>התתר</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>אשר</td>\n",
       "      <td>אשר</td>\n",
       "      <td>CC</td>\n",
       "      <td>CC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>rcmod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>אשר</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>היה</td>\n",
       "      <td>היה</td>\n",
       "      <td>COP</td>\n",
       "      <td>COP</td>\n",
       "      <td>gen=M|num=S|per=3</td>\n",
       "      <td>2</td>\n",
       "      <td>relcomp</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>היה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>במעקקא</td>\n",
       "      <td>במעקקא</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NNP</td>\n",
       "      <td>gen=F|gen=M|num=S</td>\n",
       "      <td>5</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>במעקקא</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ו</td>\n",
       "      <td>ו</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>והשתטח</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    form   lemma upostag xpostag              feats  head   deprel deps  \\\n",
       "0   1    התתר    התתר      NN      NN        gen=M|num=S    12     subj    _   \n",
       "1   2     אשר     אשר      CC      CC                NaN     1    rcmod    _   \n",
       "2   3     היה     היה     COP     COP  gen=M|num=S|per=3     2  relcomp    _   \n",
       "3   4  במעקקא  במעקקא     NNP     NNP  gen=F|gen=M|num=S     5     ROOT    _   \n",
       "4   5       ו       ו    CONJ    CONJ                NaN     3     ROOT    _   \n",
       "\n",
       "  misc  sent_id token_id token_str  \n",
       "0    _        1        1      התתר  \n",
       "1    _        1        2       אשר  \n",
       "2    _        1        3       היה  \n",
       "3    _        1        4    במעקקא  \n",
       "4    _        1        5    והשתטח  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yap_out_ft = bclm.read_yap_output(treebank_set=None, tokens_path=tokens_path, \n",
    "                                     dep_path=conll_out_ft,\n",
    "                                     map_path=map_out_ft)\n",
    "yap_out_ft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align Multitok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_merge_bio_labels(multitok_sents, tokmorph_sents, verbose=False):\n",
    "    new_sents = []\n",
    "    for (i, mt_sent), (sent_id, mor_sent) in zip(multitok_sents.iteritems(), tokmorph_sents.iteritems()):\n",
    "        new_sent = []\n",
    "        for (form, bio), (token_id, token_str, forms) in zip(mt_sent, mor_sent):\n",
    "            forms = forms.split('^')\n",
    "            bio = bio.split('^')\n",
    "            if len(forms) == len(bio):\n",
    "                new_forms = (1, list(zip(forms,bio)))\n",
    "            elif len(forms)>len(bio):\n",
    "                dif = len(forms) - len(bio)\n",
    "                new_forms = (2, list(zip(forms[:dif],['O']*dif)) + list(zip(forms[::-1], bio[::-1]))[::-1])\n",
    "                if verbose:\n",
    "                    print(new_forms)\n",
    "            else:\n",
    "                new_forms = (3, list(zip(forms[::-1], bio[::-1]))[::-1])\n",
    "                if verbose:\n",
    "                    print(new_forms)\n",
    "            new_sent.extend(new_forms[1])\n",
    "        new_sents.append(new_sent)\n",
    "    return new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_multitok(ner_pred_path, tokens_path, conll_path, map_path, output_path):\n",
    "    x = nem.read_file_sents(ner_pred_path, fix_multi_tag=False)\n",
    "    prun_yo = bclm.read_yap_output(treebank_set=None, tokens_path=tokens_path, dep_path=conll_path, map_path=map_path)\n",
    "    prun_yo = bclm.get_token_df(prun_yo, fields=['form'])\n",
    "    prun_sents = bclm.get_sentences_list(prun_yo, fields=['token_id', 'token_str', 'form'])\n",
    "    new_sents = soft_merge_bio_labels(x, prun_sents, verbose=False)\n",
    "\n",
    "    with open(output_path, 'w') as of:\n",
    "        for sent in new_sents:\n",
    "            for form, bio in sent:\n",
    "                of.write(form+' '+bio+'\\n')\n",
    "            of.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_ner_path=pruned_lat_path.replace('.lattices', '.bmes')\n",
    "if True: #not os.path.exists(pruned_ner_path):\n",
    "    align_multitok(ner_multi_path, \n",
    "                   tokens_path, \n",
    "                   conll_out,\n",
    "                   map_out,\n",
    "                   pruned_ner_path\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_ner_path_ft=pruned_lat_path_ft.replace('.lattices', '.bmes')\n",
    "if True: #not os.path.exists(pruned_ner_path):\n",
    "    align_multitok(ner_multi_path_ft, \n",
    "                   tokens_path, \n",
    "                   conll_out_ft,\n",
    "                   map_out_ft,\n",
    "                   pruned_ner_path_ft\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biose_to_o(in_path, out_path):\n",
    "    sents = 0\n",
    "    with open(out_path, 'w', encoding='utf8') as of:\n",
    "        for line in open(in_path, 'r'):\n",
    "            if line=='\\n':\n",
    "                of.write(line)\n",
    "                sents+=1\n",
    "            else:\n",
    "                line = line.strip()\n",
    "                word, tag = line.split()\n",
    "                tag = 'O'\n",
    "                of.write(word+' '+tag+'\\n')\n",
    "            \n",
    "    \n",
    "\n",
    "output_path = pruned_ner_path.replace('.bmes', '_dummy_o.bmes')\n",
    "if True: #not os.path.exists(output_path):\n",
    "    biose_to_o(pruned_ner_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_ft = pruned_ner_path_ft.replace('.bmes', '_dummy_o.bmes')\n",
    "if True: #not os.path.exists(output_path):\n",
    "    biose_to_o(pruned_ner_path_ft, output_path_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <-- NOW RUN PREDICT ON PRUNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "json_path = 'output/predict_sinai/morph_pruned_heBERT_53460.json'\n",
    "output_path = json_path.replace('.json', '.bmes')\n",
    "if not os.path.exists(output_path):\n",
    "    jsonl_to_biose(json_path, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SINGLE + MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52k\n",
      ".ipynb_checkpoints\n",
      "2k\n",
      "8k\n",
      "32k\n",
      "4k\n",
      "16k\n",
      "64k\n",
      "128k\n",
      "heBERT\n",
      "unichar_improved_52k\n",
      "unichar_improved_with_hash_52k\n",
      "distilled_52k_temp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>eval_unit</th>\n",
       "      <th>variant</th>\n",
       "      <th>prediction</th>\n",
       "      <th>align</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dev</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>-</th>\n",
       "      <td>0.795912</td>\n",
       "      <td>0.752238</td>\n",
       "      <td>0.773388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>-</th>\n",
       "      <td>0.769143</td>\n",
       "      <td>0.726319</td>\n",
       "      <td>0.747048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>-</th>\n",
       "      <td>0.712882</td>\n",
       "      <td>0.677989</td>\n",
       "      <td>0.694896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">token</th>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>0.789026</td>\n",
       "      <td>0.708016</td>\n",
       "      <td>0.746213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>0.787436</td>\n",
       "      <td>0.740414</td>\n",
       "      <td>0.763101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">test</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>-</th>\n",
       "      <td>0.736141</td>\n",
       "      <td>0.764056</td>\n",
       "      <td>0.749770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>-</th>\n",
       "      <td>0.706601</td>\n",
       "      <td>0.723730</td>\n",
       "      <td>0.715005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>-</th>\n",
       "      <td>0.633964</td>\n",
       "      <td>0.644725</td>\n",
       "      <td>0.639237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">token</th>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>0.748274</td>\n",
       "      <td>0.718294</td>\n",
       "      <td>0.732886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>0.719970</td>\n",
       "      <td>0.738215</td>\n",
       "      <td>0.728903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                p         r         f\n",
       "set  eval_unit variant prediction align                              \n",
       "dev  morph     morph   gold       -      0.795912  0.752238  0.773388\n",
       "                       hybrid     -      0.769143  0.726319  0.747048\n",
       "                       yap        -      0.712882  0.677989  0.694896\n",
       "     token     multi   tokens     -      0.789026  0.708016  0.746213\n",
       "               single  tokens     -      0.787436  0.740414  0.763101\n",
       "test morph     morph   gold       -      0.736141  0.764056  0.749770\n",
       "                       hybrid     -      0.706601  0.723730  0.715005\n",
       "                       yap        -      0.633964  0.644725  0.639237\n",
       "     token     multi   tokens     -      0.748274  0.718294  0.732886\n",
       "               single  tokens     -      0.719970  0.738215  0.728903"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for trans in os.scandir('output/sinai'):\n",
    "    trans_name = trans.name\n",
    "    print(trans_name)\n",
    "    for folder in os.scandir(trans):\n",
    "        if '.ipynb' in folder.name:\n",
    "            continue\n",
    "\n",
    "        variant, seed = folder.name.split('_')\n",
    "\n",
    "        if 'single' in folder.name:    \n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/token_gold_test_fix.bmes', \n",
    "                                       os.path.join(folder.path,'token_gold_test_fix.bmes'), str_join_char='')\n",
    "            res.append(('test', 'token', variant, 'tokens', '-', trans_name, seed, p, r, f))\n",
    "\n",
    "\n",
    "        if 'multi' in folder.name:\n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/token_gold_test_fix.bmes', \n",
    "                                       os.path.join(folder.path,'token_gold_test_dummy_o.bmes'), str_join_char='')\n",
    "            res.append(('test', 'token', variant, 'tokens', '-', trans_name, seed, p, r, f))\n",
    "\n",
    "        if 'morph' in folder.name:\n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/morph_gold_test.bmes', \n",
    "                                       os.path.join(folder.path,'morph_gold_test.bmes'), str_join_char='')\n",
    "            res.append(('test', 'morph', variant, 'gold', '-', trans_name, seed, p, r, f))\n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/morph_gold_test.bmes', \n",
    "                                       os.path.join(folder.path,'morph_yap_test.bmes'), str_join_char='')\n",
    "            res.append(('test', 'morph', variant, 'yap', '-', trans_name, seed, p, r, f))\n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/morph_gold_test.bmes', \n",
    "                                       os.path.join(folder.path,'morph_pruned_test.bmes'), str_join_char='')\n",
    "            res.append(('test', 'morph', variant, 'hybrid', '-', trans_name, seed, p, r, f))\n",
    "\n",
    "        #dev\n",
    "\n",
    "        if 'single' in folder.name:    \n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/token_gold_dev_fix.bmes', \n",
    "                                       os.path.join(folder.path,'token_gold_dev_fix.bmes'), str_join_char='')\n",
    "            res.append(('dev', 'token', variant, 'tokens', '-', trans_name, seed, p, r, f))\n",
    "\n",
    "\n",
    "        if 'multi' in folder.name:\n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/token_gold_dev_fix.bmes', \n",
    "                                       os.path.join(folder.path,'token_gold_dev_dummy_o.bmes'), str_join_char='')\n",
    "            res.append(('dev', 'token', variant, 'tokens', '-', trans_name, seed, p, r, f))\n",
    "\n",
    "        if 'morph' in folder.name:\n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/morph_gold_dev.bmes', \n",
    "                                       os.path.join(folder.path,'morph_gold_dev.bmes'), str_join_char='')\n",
    "            res.append(('dev', 'morph', variant, 'gold', '-', trans_name, seed, p, r, f))\n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/morph_gold_dev.bmes', \n",
    "                                       os.path.join(folder.path,'morph_yap_dev.bmes'), str_join_char='')\n",
    "            res.append(('dev', 'morph', variant, 'yap', '-', trans_name, seed, p, r, f))\n",
    "            p,r,f = nem.evaluate_files('../NER/data/for_ncrf/morph_gold_dev.bmes', \n",
    "                                       os.path.join(folder.path,'morph_pruned_dev.bmes'), str_join_char='')\n",
    "            res.append(('dev', 'morph', variant, 'hybrid', '-', trans_name, seed, p, r, f))\n",
    "    \n",
    "    \n",
    "\n",
    "ne_df = pd.DataFrame(res, columns=['set', 'eval_unit', 'variant', 'prediction', 'align', 'trans_name', 'seed', 'p', 'r', 'f'])\n",
    "\n",
    "ne_df.groupby(['set', 'eval_unit','variant', 'prediction', 'align'])[['p', 'r', 'f']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Level Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "o_re = re.compile('^O+$') \n",
    "s_re = re.compile('^O*SO*$|^O*BI*EO*$')\n",
    "b_re = re.compile('^O*BI*$')\n",
    "i_re = re.compile('^I+$')\n",
    "e_re = re.compile('^I*EO*$')\n",
    "def get_fixed_for_valid_biose(bio_seq):\n",
    "    if o_re.match(bio_seq):\n",
    "        return 'O'\n",
    "    if s_re.match(bio_seq):\n",
    "        return 'S'\n",
    "    if b_re.match(bio_seq):\n",
    "        return 'B'\n",
    "    if i_re.match(bio_seq):\n",
    "        return 'I'\n",
    "    if e_re.match(bio_seq):\n",
    "        return 'E'\n",
    "    raise ValueError\n",
    "    \n",
    "\n",
    "def get_fixed_for_invalid_biose(parts):\n",
    "    bio = 'O'\n",
    "    if 'S' in parts:\n",
    "        bio = 'S'\n",
    "    elif 'B' in parts and 'E' in parts:\n",
    "        bio='S'\n",
    "    elif 'E' in parts:\n",
    "        bio = 'E'\n",
    "    elif 'B' in parts:\n",
    "        bio = 'B'\n",
    "    elif 'I' in parts:\n",
    "        bio = 'I'\n",
    "    return bio\n",
    "\n",
    "valid_bio_re = re.compile('^O*BI*$|^O*BI*EO*$|^I+$|^I*EO*$|^O*SO*$')\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "def validate_biose_sequence(full_bio_seq):\n",
    "    #print(full_bio_seq)\n",
    "    bio_seq, type_seq = zip(*[('O', None) if b=='O' else b.split('-') for b in full_bio_seq])\n",
    "    bio_seq = ''.join(bio_seq)\n",
    "    valid_bio = valid_bio_re.match(bio_seq)\n",
    "    type_seq = list(filter(lambda x: x is not None, type_seq))\n",
    "    type_seq_set = set(type_seq)\n",
    "\n",
    "    if valid_bio:\n",
    "        fixed_bio = get_fixed_for_valid_biose(bio_seq)\n",
    "        if fixed_bio!='O':\n",
    "            fixed_bio += '-' + type_seq[0]\n",
    "            \n",
    "    else:\n",
    "        #take the first BIOSE tag which is not O:\n",
    "        #fixed_bio = list(filter(lambda x: x!='O', full_bio_seq))[0]\n",
    "        #rough BIOSE and first category:\n",
    "        fixed_bio = get_fixed_for_invalid_biose(bio_seq)\n",
    "        if fixed_bio!='O':\n",
    "            fixed_bio += '-' + type_seq[0]\n",
    "        \n",
    "    return valid_bio is not None, len(type_seq_set)<=1, fixed_bio\n",
    "\n",
    "\n",
    "@lru_cache(1000)\n",
    "def get_fixed_bio_sequence(full_bio_seq):\n",
    "    return validate_biose_sequence(full_bio_seq)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/danb/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dropped = [5438, 5444, 5445, 5446, 5448, 5449, 5450, 5451, 5453, 5459]\n",
    "spdf = bclm.read_dataframe('spmrl')\n",
    "spdf = spdf[(~spdf.sent_id.isin(dropped))]\n",
    "dev_gold = spdf[spdf.set=='dev']\n",
    "test_gold = spdf[spdf.set=='test']\n",
    "test_gold['sent_id'] = test_gold.sent_id.rank(method='dense').astype(int)\n",
    "dev_yap = bclm.read_yap_output(treebank_set='dev')\n",
    "test_yap = bclm.read_yap_output(treebank_set='test')\n",
    "dev_gold_sents = bclm.get_sentences_list(dev_gold, fields=['token_id', 'token_str'])\n",
    "test_gold_sents = bclm.get_sentences_list(test_gold, fields=['token_id', 'token_str'])\n",
    "dev_yap_sents = bclm.get_sentences_list(dev_yap, fields=['token_id', 'token_str'])\n",
    "test_yap_sents = bclm.get_sentences_list(test_yap, fields=['token_id', 'token_str'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/danb/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py:3509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "dev_gold_tok = (bclm.get_token_df(dev_gold, biose=['biose_layer0'])\n",
    "                .rename(columns={'biose_layer0': 'fixed_bio'}))\n",
    "test_gold_tok = (bclm.get_token_df(test_gold, biose=['biose_layer0'])\n",
    "                .rename(columns={'biose_layer0': 'fixed_bio'}))\n",
    "test_gold_tok['sent_id'] = test_gold_tok.sent_id.rank(method='dense').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_tok(path, orig_sents=dev_yap_sents):\n",
    "    x = nem.read_file_sents(path, fix_multi_tag=False)\n",
    "    new_sents = []\n",
    "    for (i, ner_sent), (sent_id, yap_sent) in zip(x.iteritems(), orig_sents.iteritems()):\n",
    "        for (form, bio), (token_id, token_str) in zip(ner_sent, yap_sent):\n",
    "            new_sents.append((sent_id, token_id, token_str, form, bio))\n",
    "    new_sents = pd.DataFrame(new_sents, columns=['sent_id', 'token_id', 'token_str', 'form', 'bio'])\n",
    "    new_toks = bclm.get_token_df(new_sents, fields=['bio'])\n",
    "    new_toks['fixed_bio'] = new_toks.bio.apply(lambda x: get_fixed_bio_sequence(tuple(x.split('^'))))\n",
    "    return new_toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_from_df(df, sent_id_col='sent_id', \n",
    "                  group_cols=['token_str'], \n",
    "                  val_cols=['fixed_bio']):\n",
    "    sents = bclm.get_sentences_list(df, fields=group_cols+val_cols)\n",
    "    return sents\n",
    "\n",
    "def evaluate_dataframes(gold_df, pred_df, fix_multi_tag_pred=True, truncate=None, ignore_cat=False, str_join_char=' '):\n",
    "    gold_sents = sents_from_df(gold_df)\n",
    "    pred_sents = sents_from_df(pred_df)\n",
    "    gold_mentions = nem.sents_to_mentions(gold_sents, truncate=truncate, ignore_cat=ignore_cat, str_join_char=str_join_char)\n",
    "    pred_mentions = nem.sents_to_mentions(pred_sents, truncate=truncate, ignore_cat=ignore_cat, str_join_char=str_join_char)\n",
    "    return nem.evaluate_mentions(gold_mentions, pred_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = '../NER/data/tokens_for_ncrf'\n",
    "dev_out = os.path.join(out_folder, 'dev_tokens.txt')\n",
    "test_out = os.path.join(out_folder, 'test_tokens.txt')\n",
    "token_paths = {'dev': dev_out, 'test': test_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(512)\n",
    "def get_prun_yo(ds, dep_path, map_path):\n",
    "\n",
    "    \n",
    "    prun_yo = bclm.read_yap_output(treebank_set=None,\n",
    "                               tokens_path=token_paths[ds],\n",
    "                               dep_path=dep_path,\n",
    "                               map_path=map_path,\n",
    "                                )\n",
    "    return prun_yo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on all pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(512)\n",
    "def get_sent_list(ds, dp, mp):\n",
    "    prun_yo = get_prun_yo(ds, dp, mp)\n",
    "    return bclm.get_sentences_list(prun_yo, fields=['token_id', 'token_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52k\n",
      ".ipynb_checkpoints\n",
      "2k\n",
      "8k\n",
      "32k\n",
      "4k\n",
      "16k\n",
      "64k\n",
      "128k\n",
      "heBERT\n",
      "unichar_improved_52k\n",
      "unichar_improved_with_hash_52k\n",
      "distilled_52k_temp\n"
     ]
    }
   ],
   "source": [
    "align_tok_res = []\n",
    "for trans in os.scandir('output/sinai'): \n",
    "    trans_name = trans.name\n",
    "    print(trans_name)\n",
    "    for folder in os.scandir(trans):\n",
    "        if 'morph' in folder.name and not '.ipynb' in folder.name:\n",
    "            ## dev \n",
    "            variant, seed = folder.name.split('_')\n",
    "            file = os.path.join(folder.path,'morph_pruned_dev.bmes')\n",
    "            multi_folder = folder.path.replace('morph_', 'multi_')\n",
    "            dep_path = os.path.join(multi_folder, 'dev_pruned.conll')\n",
    "            map_path = os.path.join(multi_folder, 'dev_pruned.map')\n",
    "            out_path = os.path.join(folder.path, 'morph_pruned_dev_align_tokens.bmes')\n",
    "\n",
    "            prun_sents = get_sent_list('dev',dep_path , map_path)\n",
    "            new_toks = get_fixed_tok(file, orig_sents=prun_sents)\n",
    "\n",
    "            if True: #not os.path.exists(out_path):\n",
    "                new_sents = bclm.get_sentences_list(new_toks, fields=['token_str', 'fixed_bio'])\n",
    "                with open(out_path, 'w') as of:\n",
    "                    for i, sent in new_sents.iteritems():\n",
    "                        for tok, bio in sent:\n",
    "                            of.write(tok+' '+bio+'\\n')\n",
    "                        of.write('\\n')\n",
    "\n",
    "            p, r, f = evaluate_dataframes(dev_gold_tok, new_toks, str_join_char='')\n",
    "\n",
    "            align_tok_res.append(('dev', 'token', 'morph', 'hybrid', 'tokens', trans_name, seed, p, r, f))\n",
    "            \n",
    "            ## test \n",
    "            variant, seed = folder.name.split('_')\n",
    "            file = os.path.join(folder.path,'morph_pruned_test.bmes')\n",
    "            multi_folder = folder.path.replace('morph_', 'multi_')\n",
    "            dep_path = os.path.join(multi_folder, 'test_pruned.conll')\n",
    "            map_path = os.path.join(multi_folder, 'test_pruned.map')\n",
    "            out_path = os.path.join(folder.path, 'morph_pruned_test_align_tokens.bmes')\n",
    "\n",
    "            prun_sents = get_sent_list('test',dep_path , map_path)\n",
    "            new_toks = get_fixed_tok(file, orig_sents=prun_sents)\n",
    "\n",
    "            if True: #not os.path.exists(out_path):\n",
    "                new_sents = bclm.get_sentences_list(new_toks, fields=['token_str', 'fixed_bio'])\n",
    "                with open(out_path, 'w') as of:\n",
    "                    for i, sent in new_sents.iteritems():\n",
    "                        for tok, bio in sent:\n",
    "                            of.write(tok+' '+bio+'\\n')\n",
    "                        of.write('\\n')\n",
    "\n",
    "            p, r, f = evaluate_dataframes(test_gold_tok, new_toks, str_join_char='')\n",
    "\n",
    "            align_tok_res.append(('test', 'token', 'morph', 'hybrid', 'tokens', trans_name, seed, p, r, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set   trans_name                      eval_unit  variant  prediction  align \n",
       "dev   128k                            token      morph    hybrid      tokens    0.792964\n",
       "      16k                             token      morph    hybrid      tokens    0.758491\n",
       "      2k                              token      morph    hybrid      tokens    0.698021\n",
       "      32k                             token      morph    hybrid      tokens    0.757407\n",
       "      4k                              token      morph    hybrid      tokens    0.727420\n",
       "      52k                             token      morph    hybrid      tokens    0.743293\n",
       "      64k                             token      morph    hybrid      tokens    0.786625\n",
       "      8k                              token      morph    hybrid      tokens    0.733904\n",
       "      distilled_52k_temp              token      morph    hybrid      tokens    0.768593\n",
       "      heBERT                          token      morph    hybrid      tokens    0.816921\n",
       "      unichar_improved_52k            token      morph    hybrid      tokens    0.772539\n",
       "      unichar_improved_with_hash_52k  token      morph    hybrid      tokens    0.750531\n",
       "test  128k                            token      morph    hybrid      tokens    0.768674\n",
       "      16k                             token      morph    hybrid      tokens    0.743010\n",
       "      2k                              token      morph    hybrid      tokens    0.666902\n",
       "      32k                             token      morph    hybrid      tokens    0.734020\n",
       "      4k                              token      morph    hybrid      tokens    0.693844\n",
       "      52k                             token      morph    hybrid      tokens    0.711603\n",
       "      64k                             token      morph    hybrid      tokens    0.764508\n",
       "      8k                              token      morph    hybrid      tokens    0.716779\n",
       "      distilled_52k_temp              token      morph    hybrid      tokens    0.732813\n",
       "      heBERT                          token      morph    hybrid      tokens    0.801168\n",
       "      unichar_improved_52k            token      morph    hybrid      tokens    0.751924\n",
       "      unichar_improved_with_hash_52k  token      morph    hybrid      tokens    0.744231\n",
       "Name: f, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_df = pd.DataFrame(align_tok_res, columns=['set', 'eval_unit', 'variant', 'prediction', 'align', 'trans_name', 'seed', 'p', 'r', 'f'])\n",
    "\n",
    "at_df.groupby(['set', 'trans_name', 'eval_unit','variant', 'prediction', 'align']).f.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all gold and YAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52k\n",
      ".ipynb_checkpoints\n",
      "2k\n",
      "8k\n",
      "32k\n",
      "4k\n",
      "16k\n",
      "64k\n",
      "128k\n",
      "heBERT\n",
      "unichar_improved_52k\n",
      "unichar_improved_with_hash_52k\n",
      "distilled_52k_temp\n"
     ]
    }
   ],
   "source": [
    "align_tok_res_yg = []\n",
    "for trans in os.scandir('output/sinai'):\n",
    "    trans_name = trans.name\n",
    "    print(trans_name)\n",
    "    for folder in os.scandir(trans):\n",
    "        if 'morph' in folder.name and not '.ipynb' in folder.name:\n",
    "            ## dev \n",
    "            ## - gold\n",
    "            variant, seed = folder.name.split('_')\n",
    "            file = os.path.join(folder.path,'morph_gold_dev.bmes')\n",
    "            out_path = os.path.join(folder.path, 'morph_gold_dev_align_tokens.bmes')\n",
    "\n",
    "            new_toks = get_fixed_tok(file, orig_sents=dev_gold_sents)\n",
    "\n",
    "            if True: #not os.path.exists(out_path):\n",
    "                new_sents = bclm.get_sentences_list(new_toks, fields=['token_str', 'fixed_bio'])\n",
    "                with open(out_path, 'w') as of:\n",
    "                    for i, sent in new_sents.iteritems():\n",
    "                        for tok, bio in sent:\n",
    "                            of.write(tok+' '+bio+'\\n')\n",
    "                        of.write('\\n')\n",
    "\n",
    "            p, r, f = evaluate_dataframes(dev_gold_tok, new_toks, str_join_char='')\n",
    "\n",
    "            align_tok_res_yg.append(('dev', 'token', 'morph', 'gold', 'tokens', trans_name, seed, p, r, f))\n",
    "\n",
    "            ## - yap\n",
    "            variant, seed = folder.name.split('_')\n",
    "            file = os.path.join(folder.path,'morph_yap_dev.bmes')\n",
    "            out_path = os.path.join(folder.path, 'morph_yap_dev_align_tokens.bmes')\n",
    "\n",
    "            new_toks = get_fixed_tok(file, orig_sents=dev_yap_sents)\n",
    "\n",
    "            if True: #not os.path.exists(out_path):\n",
    "                new_sents = bclm.get_sentences_list(new_toks, fields=['token_str', 'fixed_bio'])\n",
    "                with open(out_path, 'w') as of:\n",
    "                    for i, sent in new_sents.iteritems():\n",
    "                        for tok, bio in sent:\n",
    "                            of.write(tok+' '+bio+'\\n')\n",
    "                        of.write('\\n')\n",
    "            p, r, f = evaluate_dataframes(dev_gold_tok, new_toks, str_join_char='')\n",
    "\n",
    "            align_tok_res_yg.append(('dev', 'token', 'morph', 'yap', 'tokens', trans_name, seed, p, r, f))\n",
    "\n",
    "            ## test \n",
    "            ## - gold\n",
    "            variant, seed = folder.name.split('_')\n",
    "            file = os.path.join(folder.path,'morph_gold_test.bmes')\n",
    "            out_path = os.path.join(folder.path, 'morph_gold_test_align_tokens.bmes')\n",
    "\n",
    "            new_toks = get_fixed_tok(file, orig_sents=test_gold_sents)\n",
    "\n",
    "            if True: #not os.path.exists(out_path):\n",
    "                new_sents = bclm.get_sentences_list(new_toks, fields=['token_str', 'fixed_bio'])\n",
    "                with open(out_path, 'w') as of:\n",
    "                    for i, sent in new_sents.iteritems():\n",
    "                        for tok, bio in sent:\n",
    "                            of.write(tok+' '+bio+'\\n')\n",
    "                        of.write('\\n')\n",
    "\n",
    "            p, r, f = evaluate_dataframes(test_gold_tok, new_toks, str_join_char='')\n",
    "\n",
    "            align_tok_res_yg.append(('test', 'token', 'morph', 'gold', 'tokens', trans_name, seed, p, r, f))\n",
    "\n",
    "            ## - yap\n",
    "            variant, seed = folder.name.split('_')\n",
    "            file = os.path.join(folder.path,'morph_yap_test.bmes')\n",
    "            out_path = os.path.join(folder.path, 'morph_yap_test_align_tokens.bmes')\n",
    "\n",
    "            new_toks = get_fixed_tok(file, orig_sents=test_yap_sents)\n",
    "\n",
    "            if True: # not os.path.exists(out_path):\n",
    "                new_sents = bclm.get_sentences_list(new_toks, fields=['token_str', 'fixed_bio'])\n",
    "                with open(out_path, 'w') as of:\n",
    "                    for i, sent in new_sents.iteritems():\n",
    "                        for tok, bio in sent:\n",
    "                            of.write(tok+' '+bio+'\\n')\n",
    "                        of.write('\\n')\n",
    "            p, r, f = evaluate_dataframes(test_gold_tok, new_toks, str_join_char='')\n",
    "\n",
    "            align_tok_res_yg.append(('test', 'token', 'morph', 'yap', 'tokens', trans_name, seed, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>trans_name</th>\n",
       "      <th>eval_unit</th>\n",
       "      <th>variant</th>\n",
       "      <th>prediction</th>\n",
       "      <th>align</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dev</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">128k</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">token</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.830539</td>\n",
       "      <td>0.779559</td>\n",
       "      <td>0.804221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.820187</td>\n",
       "      <td>0.767535</td>\n",
       "      <td>0.792964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.775718</td>\n",
       "      <td>0.734269</td>\n",
       "      <td>0.754375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16k</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">token</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.810019</td>\n",
       "      <td>0.749900</td>\n",
       "      <td>0.778781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.788561</td>\n",
       "      <td>0.730661</td>\n",
       "      <td>0.758491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">test</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">unichar_improved_52k</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">token</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">morph</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.736913</td>\n",
       "      <td>0.767597</td>\n",
       "      <td>0.751924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.719576</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>0.728650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">unichar_improved_with_hash_52k</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">token</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.745432</td>\n",
       "      <td>0.787768</td>\n",
       "      <td>0.765928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.730115</td>\n",
       "      <td>0.759013</td>\n",
       "      <td>0.744231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.695627</td>\n",
       "      <td>0.712876</td>\n",
       "      <td>0.704077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                p  \\\n",
       "set  trans_name                     eval_unit variant prediction align              \n",
       "dev  128k                           token     morph   gold       tokens  0.830539   \n",
       "                                                      hybrid     tokens  0.820187   \n",
       "                                                      yap        tokens  0.775718   \n",
       "     16k                            token     morph   gold       tokens  0.810019   \n",
       "                                                      hybrid     tokens  0.788561   \n",
       "...                                                                           ...   \n",
       "test unichar_improved_52k           token     morph   hybrid     tokens  0.736913   \n",
       "                                                      yap        tokens  0.719576   \n",
       "     unichar_improved_with_hash_52k token     morph   gold       tokens  0.745432   \n",
       "                                                      hybrid     tokens  0.730115   \n",
       "                                                      yap        tokens  0.695627   \n",
       "\n",
       "                                                                                r  \\\n",
       "set  trans_name                     eval_unit variant prediction align              \n",
       "dev  128k                           token     morph   gold       tokens  0.779559   \n",
       "                                                      hybrid     tokens  0.767535   \n",
       "                                                      yap        tokens  0.734269   \n",
       "     16k                            token     morph   gold       tokens  0.749900   \n",
       "                                                      hybrid     tokens  0.730661   \n",
       "...                                                                           ...   \n",
       "test unichar_improved_52k           token     morph   hybrid     tokens  0.767597   \n",
       "                                                      yap        tokens  0.737983   \n",
       "     unichar_improved_with_hash_52k token     morph   gold       tokens  0.787768   \n",
       "                                                      hybrid     tokens  0.759013   \n",
       "                                                      yap        tokens  0.712876   \n",
       "\n",
       "                                                                                f  \n",
       "set  trans_name                     eval_unit variant prediction align             \n",
       "dev  128k                           token     morph   gold       tokens  0.804221  \n",
       "                                                      hybrid     tokens  0.792964  \n",
       "                                                      yap        tokens  0.754375  \n",
       "     16k                            token     morph   gold       tokens  0.778781  \n",
       "                                                      hybrid     tokens  0.758491  \n",
       "...                                                                           ...  \n",
       "test unichar_improved_52k           token     morph   hybrid     tokens  0.751924  \n",
       "                                                      yap        tokens  0.728650  \n",
       "     unichar_improved_with_hash_52k token     morph   gold       tokens  0.765928  \n",
       "                                                      hybrid     tokens  0.744231  \n",
       "                                                      yap        tokens  0.704077  \n",
       "\n",
       "[72 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_df = pd.DataFrame(align_tok_res+align_tok_res_yg, columns=['set', 'eval_unit', 'variant', 'prediction', 'align', 'trans_name', 'seed', 'p', 'r', 'f'])\n",
    "\n",
    "at_df.groupby(['set', 'trans_name', 'eval_unit','variant', 'prediction', 'align'])[['p', 'r', 'f']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morpheme Level Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52k\n",
      ".ipynb_checkpoints\n",
      "2k\n",
      "8k\n",
      "32k\n",
      "4k\n",
      "16k\n",
      "64k\n",
      "128k\n",
      "heBERT\n",
      "unichar_improved_52k\n",
      "unichar_improved_with_hash_52k\n",
      "distilled_52k_temp\n"
     ]
    }
   ],
   "source": [
    "align_morph_res_hyb = []\n",
    "for trans in os.scandir('output/sinai'):\n",
    "    trans_name = trans.name\n",
    "    print(trans_name)\n",
    "    for folder in os.scandir(trans):\n",
    "        if os.path.isdir(folder) and 'multi' in folder.name and not '.ipynb_checkpoints' in folder.name:\n",
    "            pruned_ner_path=os.path.join(folder.path, 'morph_pruned_dev.bmes')\n",
    "\n",
    "            p, r, f = nem.evaluate_files(decode_sets['multitok']['dev'], pruned_ner_path, str_join_char='')\n",
    "            align_morph_res_hyb.append(('dev', 'morph', 'multi', 'tokens', 'hybrid', trans_name, seed, p, r, f))\n",
    "\n",
    "            pruned_ner_path=os.path.join(folder.path, 'morph_pruned_test.bmes')\n",
    "\n",
    "            p, r, f = nem.evaluate_files(decode_sets['multitok']['test'], pruned_ner_path, str_join_char='')\n",
    "            align_morph_res_hyb.append(('test', 'morph', 'multi', 'tokens', 'hybrid', trans_name, seed, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>trans_name</th>\n",
       "      <th>eval_unit</th>\n",
       "      <th>variant</th>\n",
       "      <th>prediction</th>\n",
       "      <th>align</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dev</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">128k</th>\n",
       "      <th>morph</th>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>0.814822</td>\n",
       "      <td>0.742285</td>\n",
       "      <td>0.776844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">token</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.830539</td>\n",
       "      <td>0.779559</td>\n",
       "      <td>0.804221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.820187</td>\n",
       "      <td>0.767535</td>\n",
       "      <td>0.792964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.775718</td>\n",
       "      <td>0.734269</td>\n",
       "      <td>0.754375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16k</th>\n",
       "      <th>morph</th>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>0.796010</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>0.745671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">test</th>\n",
       "      <th>unichar_improved_52k</th>\n",
       "      <th>token</th>\n",
       "      <th>morph</th>\n",
       "      <th>yap</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.719576</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>0.728650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">unichar_improved_with_hash_52k</th>\n",
       "      <th>morph</th>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>0.685187</td>\n",
       "      <td>0.626609</td>\n",
       "      <td>0.654505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">token</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.745432</td>\n",
       "      <td>0.787768</td>\n",
       "      <td>0.765928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.730115</td>\n",
       "      <td>0.759013</td>\n",
       "      <td>0.744231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>tokens</th>\n",
       "      <td>0.695627</td>\n",
       "      <td>0.712876</td>\n",
       "      <td>0.704077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                p  \\\n",
       "set  trans_name                     eval_unit variant prediction align              \n",
       "dev  128k                           morph     multi   tokens     hybrid  0.814822   \n",
       "                                    token     morph   gold       tokens  0.830539   \n",
       "                                                      hybrid     tokens  0.820187   \n",
       "                                                      yap        tokens  0.775718   \n",
       "     16k                            morph     multi   tokens     hybrid  0.796010   \n",
       "...                                                                           ...   \n",
       "test unichar_improved_52k           token     morph   yap        tokens  0.719576   \n",
       "     unichar_improved_with_hash_52k morph     multi   tokens     hybrid  0.685187   \n",
       "                                    token     morph   gold       tokens  0.745432   \n",
       "                                                      hybrid     tokens  0.730115   \n",
       "                                                      yap        tokens  0.695627   \n",
       "\n",
       "                                                                                r  \\\n",
       "set  trans_name                     eval_unit variant prediction align              \n",
       "dev  128k                           morph     multi   tokens     hybrid  0.742285   \n",
       "                                    token     morph   gold       tokens  0.779559   \n",
       "                                                      hybrid     tokens  0.767535   \n",
       "                                                      yap        tokens  0.734269   \n",
       "     16k                            morph     multi   tokens     hybrid  0.701403   \n",
       "...                                                                           ...   \n",
       "test unichar_improved_52k           token     morph   yap        tokens  0.737983   \n",
       "     unichar_improved_with_hash_52k morph     multi   tokens     hybrid  0.626609   \n",
       "                                    token     morph   gold       tokens  0.787768   \n",
       "                                                      hybrid     tokens  0.759013   \n",
       "                                                      yap        tokens  0.712876   \n",
       "\n",
       "                                                                                f  \n",
       "set  trans_name                     eval_unit variant prediction align             \n",
       "dev  128k                           morph     multi   tokens     hybrid  0.776844  \n",
       "                                    token     morph   gold       tokens  0.804221  \n",
       "                                                      hybrid     tokens  0.792964  \n",
       "                                                      yap        tokens  0.754375  \n",
       "     16k                            morph     multi   tokens     hybrid  0.745671  \n",
       "...                                                                           ...  \n",
       "test unichar_improved_52k           token     morph   yap        tokens  0.728650  \n",
       "     unichar_improved_with_hash_52k morph     multi   tokens     hybrid  0.654505  \n",
       "                                    token     morph   gold       tokens  0.765928  \n",
       "                                                      hybrid     tokens  0.744231  \n",
       "                                                      yap        tokens  0.704077  \n",
       "\n",
       "[96 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_df = pd.DataFrame(align_tok_res+align_tok_res_yg+align_morph_res_hyb, columns=['set', 'eval_unit', 'variant', 'prediction', 'align', 'trans_name', 'seed', 'p', 'r', 'f'])\n",
    "\n",
    "at_df.groupby(['set', 'trans_name', 'eval_unit','variant', 'prediction', 'align'])[['p', 'r', 'f']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAP + GOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_multitok_yg(ner_pred_path, prun_sents, output_path):\n",
    "    x = nem.read_file_sents(ner_pred_path, fix_multi_tag=False)\n",
    "\n",
    "    new_sents = soft_merge_bio_labels(x, prun_sents, verbose=False)\n",
    "\n",
    "    with open(output_path, 'w') as of:\n",
    "        for sent in new_sents:\n",
    "            for form, bio in sent:\n",
    "                of.write(form+' '+bio+'\\n')\n",
    "            of.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_morph = {'dev': dev_gold, 'test': test_gold}\n",
    "def get_sents_for_mult(treebank_set, gold=False, pred_set=None, \n",
    "                       dep_path=None, map_path=None):\n",
    "    if treebank_set is None:\n",
    "        prun_yo = get_prun_yo(pred_set, dep_path, map_path)\n",
    "    else:\n",
    "        if not gold:\n",
    "            prun_yo = bclm.read_yap_output(treebank_set=treebank_set)\n",
    "        else:\n",
    "            prun_yo = gold_morph[treebank_set]\n",
    "    prun_yo = bclm.get_token_df(prun_yo, fields=['form'])\n",
    "    prun_sents = bclm.get_sentences_list(prun_yo, fields=['token_id', 'token_str', 'form'])\n",
    "    return prun_sents\n",
    "\n",
    "dev_yap_sents_m = get_sents_for_mult('dev')\n",
    "test_yap_sents_m = get_sents_for_mult('test')\n",
    "dev_gold_sents_m = get_sents_for_mult('dev', gold=True)\n",
    "test_gold_sents_m = get_sents_for_mult('test', gold=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52k\n",
      ".ipynb_checkpoints\n",
      "2k\n",
      "8k\n",
      "32k\n",
      "4k\n",
      "16k\n",
      "64k\n",
      "128k\n",
      "heBERT\n",
      "unichar_improved_52k\n",
      "unichar_improved_with_hash_52k\n",
      "distilled_52k_temp\n"
     ]
    }
   ],
   "source": [
    "align_morph_res_yap = []\n",
    "for trans in os.scandir('output/sinai'):\n",
    "    trans_name = trans.name\n",
    "    print(trans_name)\n",
    "    for folder in os.scandir(trans):\n",
    "        if os.path.isdir(folder) and 'multi' in folder.name and not '.ipynb_checkpoints' in folder.name:\n",
    "            #dev\n",
    "            yap_ner_path=os.path.join(folder.path, 'morph_yap_dev.bmes')\n",
    "\n",
    "            align_multitok_yg(os.path.join(folder.path, 'token_gold_dev_dummy_o.bmes'), \n",
    "                               dev_yap_sents_m,\n",
    "                               yap_ner_path\n",
    "                              )\n",
    "            p, r, f = nem.evaluate_files(decode_sets['multitok']['dev'], yap_ner_path, str_join_char='')\n",
    "            align_morph_res_yap.append(('dev', 'morph', 'multi', 'tokens', 'yap', trans_name, seed, p, r, f))\n",
    "            \n",
    "            #test\n",
    "            yap_ner_path=os.path.join(folder.path, 'morph_yap_test.bmes')\n",
    "\n",
    "            align_multitok_yg(os.path.join(folder.path, 'token_gold_test_dummy_o.bmes'), \n",
    "                               test_yap_sents_m,\n",
    "                               yap_ner_path\n",
    "                              )\n",
    "            p, r, f = nem.evaluate_files(decode_sets['multitok']['test'], yap_ner_path, str_join_char='')\n",
    "            align_morph_res_yap.append(('test', 'morph', 'multi', 'tokens', 'yap', trans_name, seed, p, r, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52k\n",
      ".ipynb_checkpoints\n",
      "2k\n",
      "8k\n",
      "32k\n",
      "4k\n",
      "16k\n",
      "64k\n",
      "128k\n",
      "heBERT\n",
      "unichar_improved_52k\n",
      "unichar_improved_with_hash_52k\n",
      "distilled_52k_temp\n"
     ]
    }
   ],
   "source": [
    "align_morph_res_gold = []\n",
    "for trans in os.scandir('output/sinai'):\n",
    "    trans_name = trans.name\n",
    "    print(trans_name)\n",
    "    for folder in os.scandir(trans):\n",
    "        if os.path.isdir(folder) and 'multi' in folder.name and not '.ipynb_checkpoints' in folder.name:\n",
    "            #dev\n",
    "            gold_ner_path=os.path.join(folder.path, 'morph_gold_dev.bmes')\n",
    "\n",
    "            align_multitok_yg(os.path.join(folder.path, 'token_gold_dev_dummy_o.bmes'), \n",
    "                               dev_gold_sents_m,\n",
    "                               gold_ner_path\n",
    "                              )\n",
    "            p, r, f = nem.evaluate_files(decode_sets['multitok']['dev'], gold_ner_path, str_join_char='')\n",
    "            align_morph_res_gold.append(('dev', 'morph', 'multi', 'tokens', 'gold', trans_name, seed, p, r, f))\n",
    "\n",
    "            #test\n",
    "            gold_ner_path=os.path.join(folder.path, 'morph_gold_test.bmes')\n",
    "\n",
    "            align_multitok_yg(os.path.join(folder.path, 'token_gold_test_dummy_o.bmes'), \n",
    "                               test_gold_sents_m,\n",
    "                               gold_ner_path\n",
    "                              )\n",
    "            p, r, f = nem.evaluate_files(decode_sets['multitok']['test'], gold_ner_path, str_join_char='')\n",
    "            align_morph_res_gold.append(('test', 'morph', 'multi', 'tokens', 'gold', trans_name, seed, p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_df = pd.DataFrame(align_tok_res+align_tok_res_yg+align_morph_res_hyb+align_morph_res_yap+align_morph_res_gold, columns=['set', 'eval_unit', 'variant', \n",
    "                                                                                                                           'prediction', 'align', 'trans_name', \n",
    "                                                                                                                           'seed', 'p', 'r', 'f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([at_df, ne_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>trans_name</th>\n",
       "      <th>heBERT</th>\n",
       "      <th>2k</th>\n",
       "      <th>4k</th>\n",
       "      <th>8k</th>\n",
       "      <th>16k</th>\n",
       "      <th>32k</th>\n",
       "      <th>52k</th>\n",
       "      <th>64k</th>\n",
       "      <th>128k</th>\n",
       "      <th>unichar_improved_52k</th>\n",
       "      <th>unichar_improved_with_hash_52k</th>\n",
       "      <th>distilled_52k_temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>eval_unit</th>\n",
       "      <th>variant</th>\n",
       "      <th>prediction</th>\n",
       "      <th>align</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">dev</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">morph</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>-</th>\n",
       "      <td>81.835690</td>\n",
       "      <td>71.832022</td>\n",
       "      <td>74.332451</td>\n",
       "      <td>75.303345</td>\n",
       "      <td>77.836627</td>\n",
       "      <td>77.005951</td>\n",
       "      <td>76.029966</td>\n",
       "      <td>80.049914</td>\n",
       "      <td>79.388250</td>\n",
       "      <td>78.948506</td>\n",
       "      <td>78.060047</td>\n",
       "      <td>77.442353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>-</th>\n",
       "      <td>80.452674</td>\n",
       "      <td>68.806994</td>\n",
       "      <td>72.245034</td>\n",
       "      <td>72.690626</td>\n",
       "      <td>74.975752</td>\n",
       "      <td>74.340257</td>\n",
       "      <td>73.421568</td>\n",
       "      <td>77.308677</td>\n",
       "      <td>77.640584</td>\n",
       "      <td>75.317802</td>\n",
       "      <td>73.671350</td>\n",
       "      <td>75.586166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>-</th>\n",
       "      <td>73.208328</td>\n",
       "      <td>65.803361</td>\n",
       "      <td>67.441296</td>\n",
       "      <td>67.338670</td>\n",
       "      <td>69.286417</td>\n",
       "      <td>68.596986</td>\n",
       "      <td>67.277702</td>\n",
       "      <td>71.746003</td>\n",
       "      <td>70.618044</td>\n",
       "      <td>72.498128</td>\n",
       "      <td>70.164699</td>\n",
       "      <td>69.895083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">multi</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">tokens</th>\n",
       "      <th>gold</th>\n",
       "      <td>80.180944</td>\n",
       "      <td>71.283581</td>\n",
       "      <td>73.183299</td>\n",
       "      <td>74.678721</td>\n",
       "      <td>75.792306</td>\n",
       "      <td>74.758317</td>\n",
       "      <td>72.184387</td>\n",
       "      <td>76.960361</td>\n",
       "      <td>78.723405</td>\n",
       "      <td>69.006655</td>\n",
       "      <td>67.246843</td>\n",
       "      <td>75.858374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <td>79.195641</td>\n",
       "      <td>69.864580</td>\n",
       "      <td>71.960541</td>\n",
       "      <td>73.460755</td>\n",
       "      <td>74.567084</td>\n",
       "      <td>73.653733</td>\n",
       "      <td>71.356746</td>\n",
       "      <td>75.849780</td>\n",
       "      <td>77.684371</td>\n",
       "      <td>67.896153</td>\n",
       "      <td>66.549135</td>\n",
       "      <td>74.844583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <td>75.129157</td>\n",
       "      <td>67.069537</td>\n",
       "      <td>68.681349</td>\n",
       "      <td>70.084404</td>\n",
       "      <td>71.517485</td>\n",
       "      <td>70.301198</td>\n",
       "      <td>67.857737</td>\n",
       "      <td>72.118344</td>\n",
       "      <td>73.518680</td>\n",
       "      <td>65.306475</td>\n",
       "      <td>63.556439</td>\n",
       "      <td>71.124813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">token</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>tokens</th>\n",
       "      <td>82.411615</td>\n",
       "      <td>71.945562</td>\n",
       "      <td>74.653507</td>\n",
       "      <td>75.467997</td>\n",
       "      <td>77.878077</td>\n",
       "      <td>77.872582</td>\n",
       "      <td>76.112238</td>\n",
       "      <td>80.583680</td>\n",
       "      <td>80.422059</td>\n",
       "      <td>79.356387</td>\n",
       "      <td>78.100574</td>\n",
       "      <td>78.095895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>tokens</th>\n",
       "      <td>81.692126</td>\n",
       "      <td>69.802140</td>\n",
       "      <td>72.742023</td>\n",
       "      <td>73.390441</td>\n",
       "      <td>75.849078</td>\n",
       "      <td>75.740698</td>\n",
       "      <td>74.329329</td>\n",
       "      <td>78.662476</td>\n",
       "      <td>79.296433</td>\n",
       "      <td>77.253904</td>\n",
       "      <td>75.053053</td>\n",
       "      <td>76.859293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>tokens</th>\n",
       "      <td>79.021235</td>\n",
       "      <td>68.559928</td>\n",
       "      <td>70.312438</td>\n",
       "      <td>70.193658</td>\n",
       "      <td>72.525007</td>\n",
       "      <td>72.762894</td>\n",
       "      <td>71.012144</td>\n",
       "      <td>75.138418</td>\n",
       "      <td>75.437486</td>\n",
       "      <td>74.869642</td>\n",
       "      <td>72.819262</td>\n",
       "      <td>73.873340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>81.282045</td>\n",
       "      <td>71.219870</td>\n",
       "      <td>73.148684</td>\n",
       "      <td>74.694507</td>\n",
       "      <td>75.978079</td>\n",
       "      <td>75.769232</td>\n",
       "      <td>72.619302</td>\n",
       "      <td>77.691137</td>\n",
       "      <td>79.638059</td>\n",
       "      <td>69.309411</td>\n",
       "      <td>67.331539</td>\n",
       "      <td>76.773382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>83.238133</td>\n",
       "      <td>73.234811</td>\n",
       "      <td>74.511338</td>\n",
       "      <td>75.766431</td>\n",
       "      <td>77.574877</td>\n",
       "      <td>76.243389</td>\n",
       "      <td>74.642975</td>\n",
       "      <td>79.354722</td>\n",
       "      <td>81.297700</td>\n",
       "      <td>71.126547</td>\n",
       "      <td>71.193356</td>\n",
       "      <td>77.537508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">test</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">morph</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>-</th>\n",
       "      <td>80.914174</td>\n",
       "      <td>68.311061</td>\n",
       "      <td>70.498517</td>\n",
       "      <td>72.394565</td>\n",
       "      <td>75.542427</td>\n",
       "      <td>75.444255</td>\n",
       "      <td>73.208474</td>\n",
       "      <td>77.463386</td>\n",
       "      <td>78.097692</td>\n",
       "      <td>76.816992</td>\n",
       "      <td>76.133866</td>\n",
       "      <td>74.898437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>-</th>\n",
       "      <td>78.235342</td>\n",
       "      <td>64.691542</td>\n",
       "      <td>67.289280</td>\n",
       "      <td>69.232216</td>\n",
       "      <td>72.326490</td>\n",
       "      <td>71.751455</td>\n",
       "      <td>69.538639</td>\n",
       "      <td>74.470594</td>\n",
       "      <td>74.902633</td>\n",
       "      <td>72.186402</td>\n",
       "      <td>71.604362</td>\n",
       "      <td>71.776715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>-</th>\n",
       "      <td>68.515912</td>\n",
       "      <td>58.730500</td>\n",
       "      <td>59.175499</td>\n",
       "      <td>61.908504</td>\n",
       "      <td>64.153693</td>\n",
       "      <td>64.348744</td>\n",
       "      <td>62.135871</td>\n",
       "      <td>66.411978</td>\n",
       "      <td>65.666843</td>\n",
       "      <td>67.129935</td>\n",
       "      <td>64.907324</td>\n",
       "      <td>63.999965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">multi</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">tokens</th>\n",
       "      <th>gold</th>\n",
       "      <td>80.094405</td>\n",
       "      <td>67.710708</td>\n",
       "      <td>69.872666</td>\n",
       "      <td>71.701280</td>\n",
       "      <td>73.548163</td>\n",
       "      <td>74.465813</td>\n",
       "      <td>71.210210</td>\n",
       "      <td>76.171415</td>\n",
       "      <td>78.362580</td>\n",
       "      <td>68.478189</td>\n",
       "      <td>67.117812</td>\n",
       "      <td>73.451373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <td>77.853912</td>\n",
       "      <td>65.321163</td>\n",
       "      <td>67.581264</td>\n",
       "      <td>69.141797</td>\n",
       "      <td>71.120494</td>\n",
       "      <td>72.641682</td>\n",
       "      <td>69.400636</td>\n",
       "      <td>74.467660</td>\n",
       "      <td>76.794042</td>\n",
       "      <td>66.777591</td>\n",
       "      <td>65.450549</td>\n",
       "      <td>71.832052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <td>71.939464</td>\n",
       "      <td>62.000360</td>\n",
       "      <td>63.143009</td>\n",
       "      <td>64.730640</td>\n",
       "      <td>66.452104</td>\n",
       "      <td>66.921270</td>\n",
       "      <td>64.328458</td>\n",
       "      <td>68.719760</td>\n",
       "      <td>71.090779</td>\n",
       "      <td>62.563216</td>\n",
       "      <td>61.196780</td>\n",
       "      <td>66.223277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">token</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>tokens</th>\n",
       "      <td>81.338137</td>\n",
       "      <td>68.775084</td>\n",
       "      <td>70.982110</td>\n",
       "      <td>73.032510</td>\n",
       "      <td>75.989217</td>\n",
       "      <td>75.843706</td>\n",
       "      <td>73.631309</td>\n",
       "      <td>78.033365</td>\n",
       "      <td>78.497625</td>\n",
       "      <td>77.364710</td>\n",
       "      <td>76.592837</td>\n",
       "      <td>75.192894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>tokens</th>\n",
       "      <td>80.116794</td>\n",
       "      <td>66.690170</td>\n",
       "      <td>69.384361</td>\n",
       "      <td>71.677928</td>\n",
       "      <td>74.300988</td>\n",
       "      <td>73.402028</td>\n",
       "      <td>71.160281</td>\n",
       "      <td>76.450760</td>\n",
       "      <td>76.867436</td>\n",
       "      <td>75.192405</td>\n",
       "      <td>74.423127</td>\n",
       "      <td>73.281313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>tokens</th>\n",
       "      <td>76.546054</td>\n",
       "      <td>64.860649</td>\n",
       "      <td>66.125232</td>\n",
       "      <td>68.478728</td>\n",
       "      <td>70.855734</td>\n",
       "      <td>70.879543</td>\n",
       "      <td>68.194652</td>\n",
       "      <td>72.573070</td>\n",
       "      <td>72.721969</td>\n",
       "      <td>72.864983</td>\n",
       "      <td>70.407696</td>\n",
       "      <td>69.649269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>80.405095</td>\n",
       "      <td>68.016143</td>\n",
       "      <td>70.299939</td>\n",
       "      <td>72.061328</td>\n",
       "      <td>73.790260</td>\n",
       "      <td>75.219625</td>\n",
       "      <td>71.904913</td>\n",
       "      <td>77.233331</td>\n",
       "      <td>79.567732</td>\n",
       "      <td>68.948920</td>\n",
       "      <td>67.682730</td>\n",
       "      <td>74.333404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>79.597639</td>\n",
       "      <td>67.327584</td>\n",
       "      <td>71.217350</td>\n",
       "      <td>71.536644</td>\n",
       "      <td>73.428532</td>\n",
       "      <td>74.780171</td>\n",
       "      <td>71.798568</td>\n",
       "      <td>77.944046</td>\n",
       "      <td>79.328892</td>\n",
       "      <td>67.339852</td>\n",
       "      <td>65.492282</td>\n",
       "      <td>74.891896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "trans_name                                   heBERT         2k         4k  \\\n",
       "set  eval_unit variant prediction align                                     \n",
       "dev  morph     morph   gold       -       81.835690  71.832022  74.332451   \n",
       "                       hybrid     -       80.452674  68.806994  72.245034   \n",
       "                       yap        -       73.208328  65.803361  67.441296   \n",
       "               multi   tokens     gold    80.180944  71.283581  73.183299   \n",
       "                                  hybrid  79.195641  69.864580  71.960541   \n",
       "                                  yap     75.129157  67.069537  68.681349   \n",
       "     token     morph   gold       tokens  82.411615  71.945562  74.653507   \n",
       "                       hybrid     tokens  81.692126  69.802140  72.742023   \n",
       "                       yap        tokens  79.021235  68.559928  70.312438   \n",
       "               multi   tokens     -       81.282045  71.219870  73.148684   \n",
       "               single  tokens     -       83.238133  73.234811  74.511338   \n",
       "test morph     morph   gold       -       80.914174  68.311061  70.498517   \n",
       "                       hybrid     -       78.235342  64.691542  67.289280   \n",
       "                       yap        -       68.515912  58.730500  59.175499   \n",
       "               multi   tokens     gold    80.094405  67.710708  69.872666   \n",
       "                                  hybrid  77.853912  65.321163  67.581264   \n",
       "                                  yap     71.939464  62.000360  63.143009   \n",
       "     token     morph   gold       tokens  81.338137  68.775084  70.982110   \n",
       "                       hybrid     tokens  80.116794  66.690170  69.384361   \n",
       "                       yap        tokens  76.546054  64.860649  66.125232   \n",
       "               multi   tokens     -       80.405095  68.016143  70.299939   \n",
       "               single  tokens     -       79.597639  67.327584  71.217350   \n",
       "\n",
       "trans_name                                       8k        16k        32k  \\\n",
       "set  eval_unit variant prediction align                                     \n",
       "dev  morph     morph   gold       -       75.303345  77.836627  77.005951   \n",
       "                       hybrid     -       72.690626  74.975752  74.340257   \n",
       "                       yap        -       67.338670  69.286417  68.596986   \n",
       "               multi   tokens     gold    74.678721  75.792306  74.758317   \n",
       "                                  hybrid  73.460755  74.567084  73.653733   \n",
       "                                  yap     70.084404  71.517485  70.301198   \n",
       "     token     morph   gold       tokens  75.467997  77.878077  77.872582   \n",
       "                       hybrid     tokens  73.390441  75.849078  75.740698   \n",
       "                       yap        tokens  70.193658  72.525007  72.762894   \n",
       "               multi   tokens     -       74.694507  75.978079  75.769232   \n",
       "               single  tokens     -       75.766431  77.574877  76.243389   \n",
       "test morph     morph   gold       -       72.394565  75.542427  75.444255   \n",
       "                       hybrid     -       69.232216  72.326490  71.751455   \n",
       "                       yap        -       61.908504  64.153693  64.348744   \n",
       "               multi   tokens     gold    71.701280  73.548163  74.465813   \n",
       "                                  hybrid  69.141797  71.120494  72.641682   \n",
       "                                  yap     64.730640  66.452104  66.921270   \n",
       "     token     morph   gold       tokens  73.032510  75.989217  75.843706   \n",
       "                       hybrid     tokens  71.677928  74.300988  73.402028   \n",
       "                       yap        tokens  68.478728  70.855734  70.879543   \n",
       "               multi   tokens     -       72.061328  73.790260  75.219625   \n",
       "               single  tokens     -       71.536644  73.428532  74.780171   \n",
       "\n",
       "trans_name                                      52k        64k       128k  \\\n",
       "set  eval_unit variant prediction align                                     \n",
       "dev  morph     morph   gold       -       76.029966  80.049914  79.388250   \n",
       "                       hybrid     -       73.421568  77.308677  77.640584   \n",
       "                       yap        -       67.277702  71.746003  70.618044   \n",
       "               multi   tokens     gold    72.184387  76.960361  78.723405   \n",
       "                                  hybrid  71.356746  75.849780  77.684371   \n",
       "                                  yap     67.857737  72.118344  73.518680   \n",
       "     token     morph   gold       tokens  76.112238  80.583680  80.422059   \n",
       "                       hybrid     tokens  74.329329  78.662476  79.296433   \n",
       "                       yap        tokens  71.012144  75.138418  75.437486   \n",
       "               multi   tokens     -       72.619302  77.691137  79.638059   \n",
       "               single  tokens     -       74.642975  79.354722  81.297700   \n",
       "test morph     morph   gold       -       73.208474  77.463386  78.097692   \n",
       "                       hybrid     -       69.538639  74.470594  74.902633   \n",
       "                       yap        -       62.135871  66.411978  65.666843   \n",
       "               multi   tokens     gold    71.210210  76.171415  78.362580   \n",
       "                                  hybrid  69.400636  74.467660  76.794042   \n",
       "                                  yap     64.328458  68.719760  71.090779   \n",
       "     token     morph   gold       tokens  73.631309  78.033365  78.497625   \n",
       "                       hybrid     tokens  71.160281  76.450760  76.867436   \n",
       "                       yap        tokens  68.194652  72.573070  72.721969   \n",
       "               multi   tokens     -       71.904913  77.233331  79.567732   \n",
       "               single  tokens     -       71.798568  77.944046  79.328892   \n",
       "\n",
       "trans_name                                unichar_improved_52k  \\\n",
       "set  eval_unit variant prediction align                          \n",
       "dev  morph     morph   gold       -                  78.948506   \n",
       "                       hybrid     -                  75.317802   \n",
       "                       yap        -                  72.498128   \n",
       "               multi   tokens     gold               69.006655   \n",
       "                                  hybrid             67.896153   \n",
       "                                  yap                65.306475   \n",
       "     token     morph   gold       tokens             79.356387   \n",
       "                       hybrid     tokens             77.253904   \n",
       "                       yap        tokens             74.869642   \n",
       "               multi   tokens     -                  69.309411   \n",
       "               single  tokens     -                  71.126547   \n",
       "test morph     morph   gold       -                  76.816992   \n",
       "                       hybrid     -                  72.186402   \n",
       "                       yap        -                  67.129935   \n",
       "               multi   tokens     gold               68.478189   \n",
       "                                  hybrid             66.777591   \n",
       "                                  yap                62.563216   \n",
       "     token     morph   gold       tokens             77.364710   \n",
       "                       hybrid     tokens             75.192405   \n",
       "                       yap        tokens             72.864983   \n",
       "               multi   tokens     -                  68.948920   \n",
       "               single  tokens     -                  67.339852   \n",
       "\n",
       "trans_name                                unichar_improved_with_hash_52k  \\\n",
       "set  eval_unit variant prediction align                                    \n",
       "dev  morph     morph   gold       -                            78.060047   \n",
       "                       hybrid     -                            73.671350   \n",
       "                       yap        -                            70.164699   \n",
       "               multi   tokens     gold                         67.246843   \n",
       "                                  hybrid                       66.549135   \n",
       "                                  yap                          63.556439   \n",
       "     token     morph   gold       tokens                       78.100574   \n",
       "                       hybrid     tokens                       75.053053   \n",
       "                       yap        tokens                       72.819262   \n",
       "               multi   tokens     -                            67.331539   \n",
       "               single  tokens     -                            71.193356   \n",
       "test morph     morph   gold       -                            76.133866   \n",
       "                       hybrid     -                            71.604362   \n",
       "                       yap        -                            64.907324   \n",
       "               multi   tokens     gold                         67.117812   \n",
       "                                  hybrid                       65.450549   \n",
       "                                  yap                          61.196780   \n",
       "     token     morph   gold       tokens                       76.592837   \n",
       "                       hybrid     tokens                       74.423127   \n",
       "                       yap        tokens                       70.407696   \n",
       "               multi   tokens     -                            67.682730   \n",
       "               single  tokens     -                            65.492282   \n",
       "\n",
       "trans_name                                distilled_52k_temp  \n",
       "set  eval_unit variant prediction align                       \n",
       "dev  morph     morph   gold       -                77.442353  \n",
       "                       hybrid     -                75.586166  \n",
       "                       yap        -                69.895083  \n",
       "               multi   tokens     gold             75.858374  \n",
       "                                  hybrid           74.844583  \n",
       "                                  yap              71.124813  \n",
       "     token     morph   gold       tokens           78.095895  \n",
       "                       hybrid     tokens           76.859293  \n",
       "                       yap        tokens           73.873340  \n",
       "               multi   tokens     -                76.773382  \n",
       "               single  tokens     -                77.537508  \n",
       "test morph     morph   gold       -                74.898437  \n",
       "                       hybrid     -                71.776715  \n",
       "                       yap        -                63.999965  \n",
       "               multi   tokens     gold             73.451373  \n",
       "                                  hybrid           71.832052  \n",
       "                                  yap              66.223277  \n",
       "     token     morph   gold       tokens           75.192894  \n",
       "                       hybrid     tokens           73.281313  \n",
       "                       yap        tokens           69.649269  \n",
       "               multi   tokens     -                74.333404  \n",
       "               single  tokens     -                74.891896  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.groupby(['set', 'eval_unit','variant', 'prediction', 'align','trans_name']).f.mean().unstack().mul(100)[['heBERT', '2k', '4k', \n",
    "                                                                                                               '8k', '16k', '32k', '52k', '64k',\n",
    "                                                                                                               '128k', 'unichar_improved_52k', 'unichar_improved_with_hash_52k',\n",
    "                                                                                                               'distilled_52k_temp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "at_df = pd.DataFrame(align_tok_res+align_tok_res_yg+align_morph_res_hyb+align_morph_res_yap+align_morph_res_gold, columns=['set', 'eval_unit', 'variant', \n",
    "                                                                                                                           'prediction', 'align', 'trans_name', \n",
    "                                                                                                                           'seed', 'p', 'r', 'f'])\n",
    "\n",
    "(at_df.groupby(['set', 'trans_name', 'eval_unit','variant', 'prediction', 'align']).f.agg(['mean', 'std']).mul(100).round(2)\n",
    "         .assign(mean = lambda x: '$'+x['mean'].apply('{:,.2f}'.format).astype(str)+' ± '+ (1.96*(x['std']/np.sqrt(10))).round(1).astype(str)+'$')[['mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>trans_name</th>\n",
       "      <th>eval_unit</th>\n",
       "      <th>variant</th>\n",
       "      <th>prediction</th>\n",
       "      <th>align</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dev</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">128k</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>-</th>\n",
       "      <td>$79.39 ± 0.4$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>-</th>\n",
       "      <td>$77.64 ± 0.4$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>-</th>\n",
       "      <td>$70.62 ± 0.2$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">token</th>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>$79.64 ± 0.5$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>$81.30 ± 0.2$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">test</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">unichar_improved_with_hash_52k</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">morph</th>\n",
       "      <th>gold</th>\n",
       "      <th>-</th>\n",
       "      <td>$76.13 ± 0.5$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <th>-</th>\n",
       "      <td>$71.60 ± 0.7$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yap</th>\n",
       "      <th>-</th>\n",
       "      <td>$64.91 ± 0.4$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">token</th>\n",
       "      <th>multi</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>$67.68 ± 0.7$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <th>tokens</th>\n",
       "      <th>-</th>\n",
       "      <td>$65.49 ± 0.3$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 mean\n",
       "set  trans_name                     eval_unit variant prediction align               \n",
       "dev  128k                           morph     morph   gold       -      $79.39 ± 0.4$\n",
       "                                                      hybrid     -      $77.64 ± 0.4$\n",
       "                                                      yap        -      $70.62 ± 0.2$\n",
       "                                    token     multi   tokens     -      $79.64 ± 0.5$\n",
       "                                              single  tokens     -      $81.30 ± 0.2$\n",
       "...                                                                               ...\n",
       "test unichar_improved_with_hash_52k morph     morph   gold       -      $76.13 ± 0.5$\n",
       "                                                      hybrid     -      $71.60 ± 0.7$\n",
       "                                                      yap        -      $64.91 ± 0.4$\n",
       "                                    token     multi   tokens     -      $67.68 ± 0.7$\n",
       "                                              single  tokens     -      $65.49 ± 0.3$\n",
       "\n",
       "[120 rows x 1 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ne_df.groupby(['set', 'trans_name', 'eval_unit','variant', 'prediction', 'align']).f.agg(['mean', 'std']).mul(100).round(2)\n",
    "         .assign(mean = lambda x: '$'+x['mean'].apply('{:,.2f}'.format).astype(str)+' ± '+ (1.96*(x['std']/np.sqrt(10))).round(1).astype(str)+'$')[['mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>trans_name</th>\n",
       "      <th>heBERT</th>\n",
       "      <th>2k</th>\n",
       "      <th>4k</th>\n",
       "      <th>8k</th>\n",
       "      <th>16k</th>\n",
       "      <th>32k</th>\n",
       "      <th>52k</th>\n",
       "      <th>64k</th>\n",
       "      <th>128k</th>\n",
       "      <th>unichar_improved_52k</th>\n",
       "      <th>unichar_improved_with_hash_52k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>93.692111</td>\n",
       "      <td>92.755259</td>\n",
       "      <td>93.003025</td>\n",
       "      <td>93.076573</td>\n",
       "      <td>93.041299</td>\n",
       "      <td>93.306393</td>\n",
       "      <td>93.072591</td>\n",
       "      <td>93.350109</td>\n",
       "      <td>93.267056</td>\n",
       "      <td>92.265538</td>\n",
       "      <td>91.669155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>91.851358</td>\n",
       "      <td>91.010369</td>\n",
       "      <td>91.439091</td>\n",
       "      <td>91.420828</td>\n",
       "      <td>91.249540</td>\n",
       "      <td>91.523147</td>\n",
       "      <td>91.126840</td>\n",
       "      <td>91.474458</td>\n",
       "      <td>91.650169</td>\n",
       "      <td>90.204324</td>\n",
       "      <td>89.918698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "trans_name     heBERT         2k         4k         8k        16k        32k  \\\n",
       "pred_set                                                                       \n",
       "dev         93.692111  92.755259  93.003025  93.076573  93.041299  93.306393   \n",
       "test        91.851358  91.010369  91.439091  91.420828  91.249540  91.523147   \n",
       "\n",
       "trans_name        52k        64k       128k  unichar_improved_52k  \\\n",
       "pred_set                                                            \n",
       "dev         93.072591  93.350109  93.267056             92.265538   \n",
       "test        91.126840  91.474458  91.650169             90.204324   \n",
       "\n",
       "trans_name  unichar_improved_with_hash_52k  \n",
       "pred_set                                    \n",
       "dev                              91.669155  \n",
       "test                             89.918698  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_res_df.groupby(['pred_set', 'trans_name']).f_seg_pos.mean().unstack()[['heBERT', '2k', '4k', \n",
    "                                                                           '8k', '16k', '32k', '52k', '64k',\n",
    "                                                                           '128k', 'unichar_improved_52k', 'unichar_improved_with_hash_52k']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>trans_name</th>\n",
       "      <th>heBERT</th>\n",
       "      <th>2k</th>\n",
       "      <th>4k</th>\n",
       "      <th>8k</th>\n",
       "      <th>16k</th>\n",
       "      <th>32k</th>\n",
       "      <th>52k</th>\n",
       "      <th>64k</th>\n",
       "      <th>128k</th>\n",
       "      <th>unichar_improved_52k</th>\n",
       "      <th>unichar_improved_with_hash_52k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>98.075050</td>\n",
       "      <td>97.266295</td>\n",
       "      <td>97.589115</td>\n",
       "      <td>97.567593</td>\n",
       "      <td>97.490896</td>\n",
       "      <td>97.765845</td>\n",
       "      <td>97.531311</td>\n",
       "      <td>97.734921</td>\n",
       "      <td>97.714928</td>\n",
       "      <td>96.759412</td>\n",
       "      <td>96.351061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>97.963137</td>\n",
       "      <td>97.080259</td>\n",
       "      <td>97.530438</td>\n",
       "      <td>97.547616</td>\n",
       "      <td>97.386152</td>\n",
       "      <td>97.650438</td>\n",
       "      <td>97.201905</td>\n",
       "      <td>97.646012</td>\n",
       "      <td>97.786412</td>\n",
       "      <td>96.414330</td>\n",
       "      <td>96.112701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "trans_name     heBERT         2k         4k         8k        16k        32k  \\\n",
       "pred_set                                                                       \n",
       "dev         98.075050  97.266295  97.589115  97.567593  97.490896  97.765845   \n",
       "test        97.963137  97.080259  97.530438  97.547616  97.386152  97.650438   \n",
       "\n",
       "trans_name        52k        64k       128k  unichar_improved_52k  \\\n",
       "pred_set                                                            \n",
       "dev         97.531311  97.734921  97.714928             96.759412   \n",
       "test        97.201905  97.646012  97.786412             96.414330   \n",
       "\n",
       "trans_name  unichar_improved_with_hash_52k  \n",
       "pred_set                                    \n",
       "dev                              96.351061  \n",
       "test                             96.112701  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_res_df.groupby(['pred_set', 'trans_name']).f_seg_only.mean().unstack()[['heBERT', '2k', '4k', \n",
    "                                                                           '8k', '16k', '32k', '52k', '64k',\n",
    "                                                                           '128k', 'unichar_improved_52k', 'unichar_improved_with_hash_52k']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
